{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font><b>第五章 决策树</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、决策树是一种基本的分类与回归方法；\n",
    "\n",
    "2、决策树呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。\n",
    "\n",
    "3、优点：具有可读性，分类速度快。\n",
    "\n",
    "4、**决策树学习算法的3个步骤：1）特征选择；2）决策树的生成；3）决策树的修剪。**\n",
    "\n",
    "5、**决策树模型：** \n",
    "\n",
    "   * 分类决策树模型是一种描述对实例进行分类的树形结构。决策树有结点(node)和有向边(directed edge)组成。结点有2种：1）内部结点(internal node):表示一个特征或属性；2）叶子结点(leaf node): 表示一个类。\n",
    "   \n",
    "   * 决策树的损失函数通常是正则化的极大似然函数。\n",
    "   \n",
    "   * 决策树的学习策略是以损失函数为目标函数的最小化。\n",
    "   \n",
    "\n",
    "  **1）特征选择：**\n",
    "  \n",
    "  * 特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树的学习效率。\n",
    "  \n",
    "  * 如果利用一个特征分类的结果与随机分类的结果没有很大的判别，则称该特征没有分类能力，可以扔掉。\n",
    "  \n",
    "  * 通常特征选择的准则是 **信息增益**或**信息增益比**\n",
    "  \n",
    "  <font size=3><b>信息增益(information gain)：</b></font>\n",
    "  \n",
    "  * 究竟选择哪个特征更好？？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有更好的分类，那么就更应该选择这个特征。信息增益就能够很好的表示这一直观的准则。\n",
    "  \n",
    "   <font color='red'><b>a) 熵(entropy)</b></font>\n",
    "   \n",
    "   **熵**是表示随机变量不确定性的度量。设$X$是一个取有限个数的离散随机变量。其概率分布为$$P(X=x)=p_i, i=1,2,\\cdots,n$$ 则随机变量$X$的熵定义为 $H(X)=-\\sum_{i=1}^np_ilogp_i$ . 熵只依赖于$X$的分布，与$X$的取值无关。所以，$X$的熵记作$H(p)$. 即$$H(p)=-\\sum_{i=1}^np_ilogp_i$$ 熵越大，随机变量的不确定性越大。$0\\leq H(p)\\leq logn$ .\n",
    "   \n",
    "   **条件熵(conditional entropy)** $H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。$$H(Y|X)=\\sum_{i=1}^np_iH(Y|X=x_i)$$ 其中，$p_i=P(X=xi),i=1,2,\\cdots,n$\n",
    "   \n",
    "   当熵和条件熵中的概率由数据估计（特别是极大似然估计)得到时，所对应的熵与条件熵分别称为**经验熵（empirical entropy）** 和**经验条件熵（empirical conditional entropy)**\n",
    "   \n",
    "   <font color='red'><b>b) 信息增益(information gain)</b></font>\n",
    "   \n",
    "   **信息增益** 表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度。\n",
    "   \n",
    "   定义(**信息增益**) 特征A对训练数据集D的信息增益$g(D,A)$,定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即$$g(D,A) = H(D) - H(D|A)$$ 一般情况下，熵$H(Y)$与条件熵$H(D|A)$之差称为互信息(mutual information).\n",
    "   \n",
    "   根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增增最大的特征。\n",
    "   \n",
    "   **算法(信息增益的算法）**\n",
    "   \n",
    "   输入：训练数据集$D$和特征$A$\n",
    "   \n",
    "   输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$\n",
    "   \n",
    "   step1: 计算数据集$D$的经验熵$H(D)$ \n",
    "   ### $$H(D) = -\\sum_{k=1}^K\\frac{|C_k|}{D}log_2\\frac{|C_k|}{D}$$\n",
    "   \n",
    "   step2: 计算特征$A$对数据集$D$的经验熵$H(D|A)$ \n",
    "   ### $$H(D|A) = \\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|}$$\n",
    "   \n",
    "   step3: 计算信息增益\n",
    "   ### $$g(D,A) = H(D) - H(D|A)$$\n",
    "   \n",
    "   **信息增益比(information gain ratio)** 以信息增益作为划分训练数据集的特征， 存在偏向于选择取值较多的特征的问题。使用信息增益比，可以对这一问题进行校正。是特征选择的另一准则。\n",
    "   \n",
    "   定义(**信息增益比**)  特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为\n",
    "   ### $$g_R(D,A) = \\frac{g(D,A)}{H_A(D)}$$ \n",
    "   .其中，$H_A(D) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}$ ,$n$是特征$A$的取值个数。\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **2）决策树的生成：**\n",
    " \n",
    " * <font color='red'><b>ID3算法</b></font>\n",
    " \n",
    " ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。\n",
    " \n",
    " 具体方法：从根结点(root node)开始，对结点计算所有可能的特征信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立已结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。\n",
    " \n",
    "**算法（ID3算法）**\n",
    " \n",
    " 输入：训练数据集$D$，特征集$A$阈值$\\epsilon$\n",
    " \n",
    " 输出：决策树T\n",
    " \n",
    " step1: 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$\n",
    " \n",
    " step2: 若$A = \\emptyset$ , 则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$\n",
    " \n",
    " step3: 否则，按算法5.1计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$\n",
    " \n",
    " step4: 如果$A_g$的信息增益小于阈值$\\epsilon$，则$T$为单结点树，并将$D$中实例最大的类$C_k$作为该结果的类标记，返回$T$\n",
    " \n",
    " step5：否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及子结点构成树$T$，返回$T$\n",
    " \n",
    " step6: 对第$i$个子结点，以$D_i$为训练集，以$A - \\{A_g\\}$为特征集，递归地调用step1-step5，得到子树$T_i$，返回$T_i$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **3）决策树的修剪(pruning)：** 将已生成的树进行简化的过程，称为剪枝。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或你结点作为新的叶结点，从而简化分类树模型。 \n",
    "  \n",
    "  实现方法：通过极小化决策树整体的损失函数(loss function) 或代价函数（cost function)来实现。\n",
    "  \n",
    "  设树$T$的叶结点个数为$|T|$, $t$ 是树$T$的叶结点，该叶结点有$N_t$个样本点。其中$k$类的样本点有$N_{tk}$个，$k=1,2,\\cdots,K$，$H_t(T)$为叶结点$t$上的经验熵，$\\alpha\\geq 0 $为参数，则决策树的损失函数定义为：\n",
    "  #### $$\\begin{align}C_{\\alpha}(T) &= \\sum_{t=1}^{|T|}N_tH_t(T) + \\alpha |T| \\\\\n",
    "                                    &= \\sum_{t=1}^{|T|}N_t(-\\sum\\limits_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{Nt}) + \\alpha |T| \\\\\n",
    "                                    &= -\\sum_{t=1}^{|T|}\\sum\\limits_k N_{tk}log\\frac{N_{tk}}{Nt} + \\alpha |T| \n",
    "  \\end{align}$$ \n",
    "  其中，经验熵为 \n",
    "  #### $$H_t(T) = -\\sum\\limits_k \\frac{N_{tk}}{N_t}log\\frac{N_{tk}}{Nt}$$\n",
    " \n",
    " **算法（树的剪枝算法）**\n",
    " \n",
    " 输入：生成算法产生的整个树$T$, 参数$\\alpha$\n",
    " \n",
    " 输出：修剪后的子树$T_\\alpha$\n",
    " \n",
    " step1: 计算每个结点的经验熵\n",
    " \n",
    " step2: 递归地从树的叶结点向上回缩。设一组叶结点回缩到其你结点之前与之后的整体树分别为$T_B$和$T_A$, 其对应的损失函数分别为$C_\\alpha(T_B)$ 和$C_\\alpha(T_A)$，如果 $C_\\alpha(T_A) \\leq C_\\alpha(T_B)$ , 则进行剪枝，将父结点变成新的叶结点（即如果不包含该叶子结点的损失函数反而更小，那就不要这个叶子结点）\n",
    " \n",
    " step3: 返回step2, 直到不能继续为止。得到损失函数最小的子树$T_\\alpha$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4 color='blue'><b>CART算法（classification and regression tree,CART)</b></font>\n",
    "\n",
    "* <font size=3><b>step1：决策树的生成</b></font> 决策树的生成就是递归地 构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树。\n",
    "\n",
    "**1. 回归树的生成** 假设$X$与$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练集$D={(x_i,y_i)},i=1,2,\\cdots,N$，考虑如何生成回归树。\n",
    "\n",
    "* 一颗回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值。假设已将输入空间划分为$M$个单元$R_1,R_2,\\cdots,R_M$，并且在每个单元$R_M$上有一个固定的输出值 $c_m$，于是回归树模型可以表示为\n",
    "#### $$f(x) = \\sum_{m=1}^Mc_mI(x\\in R_m)$$\n",
    "\n",
    "* 怎样对输入空间进行划分？ 这里采用启发式的方法，选择第 $j$个变量 $x^{(j)}$ 和它的取值$s$, 作为切分变量（splitting variable）和切分点（splitting point）,并定义2个区域：\n",
    "#### $$R_1(j,s)=\\{x|x^{(j)}\\leq s\\} 和 R_2(j,s)=\\{x|x^{(j)} > s\\}$$ \n",
    "然后寻找最优切分变量$j$和最优切分点$s$ 。具体地，求解\n",
    "### $$\\mathop {min}\\limits_{j,s}[\\mathop {min}\\limits_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop {min}\\limits_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "对固定输入变量$j$可 以找到最优切分点$s$.\n",
    "\n",
    "#### $$\\mathop {c1}\\limits^\\wedge = ave(y_i|x_i\\in R_1(j,s)) 和 \\mathop {c2}\\limits^\\wedge = ave(y_i|x_i\\in R_2(j,s))$$\n",
    "遍历所有输入变量，找到最优的切分变量$j$，构成一个$(j,s)$。 依次将输入空间切分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。\n",
    "\n",
    "**算法（最小二乘回归树生成算法）**\n",
    "\n",
    "输入： 训练数据集$D$\n",
    "\n",
    "输出：回归树$f(x)$\n",
    "\n",
    "在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。\n",
    "\n",
    "step1: 选择最优切分变量$j$ 与切分点$s$，求解 \n",
    "### $$\\mathop {min}\\limits_{j,s}[\\mathop {min}\\limits_{c_1}\\sum\\limits_{x_i\\in R_1(j,s)}(y_i-c_1)^2 + \\mathop {min}\\limits_{c_2}\\sum\\limits_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "遍历变量$j$，对固定的切分变量$j$扫描切分点$s$,使上式达到最小值的$(j,s)$\n",
    "\n",
    "step2: 用选定的对$(j,s)$划分区域并决定相应的输出值：\n",
    "#### $$R_1(j,s) = \\{x|x^{(j)}\\leq s\\},  R_2(j,s) = \\{x|x^{(j)} > s\\} \\\\ \\mathop {c_m}\\limits^\\wedge = \\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)} y_i, x\\in R_m, m = 1,2$$\n",
    "\n",
    "step3: 继续对以上两个子区域调用step1,step2,直至满足停止条件。\n",
    "\n",
    "step4: 对输入空间划分为$M$个区域$R_1,R_2,\\cdots,R_M$,生成决策树：\n",
    "#### $$f(x) = \\sum_{m=1}^N \\mathop {c_m}\\limits^\\wedge I(x\\in R_m)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 分类树的生成** 分类树用基尼指数选择最优的特征，同时决定该特征的最优二值切分点。\n",
    "\n",
    "**定义(基尼系数)** 分类问题中，假设有$K$ 个类，样本点属于第$k$ 类的概率为$p_k$, 则概率分布的基尼指数定义为 \n",
    "#### $$Gini(p) = \\sum_{k=1}^Kp_k(1-p_k) = 1- \\sum_{k=1}^Kp_k^2$$\n",
    "对于二分类问题，若样本点属于第1个类的概率是$p$，则概率分布基尼指数为\n",
    "#### $$Gini(p) = 2p(1-p)$$\n",
    "对于给定的样本集合$D$，其基尼指数为\n",
    "#### $$Gini(D) = 1 - \\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$$\n",
    ",其中，$C_k$是$D$中属于第$k$类的样本子集，$K$是类的个数。\n",
    "\n",
    "如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即\n",
    "#### $$D_1 = \\{(x,y)\\in D|A(x) = a\\}, D_2 = D-D_1$$\n",
    "则在特征$A$的条件下，集合$D$的基尼指数定义为：\n",
    "#### $$Gini(D,A) = \\frac{|D1|}{|D|}Gini(D_1) + \\frac{|D2|}{|D|}Gini(D_2)$$\n",
    "\n",
    "基尼指数 $Gini(D)$表示集合$D$的不确定性。基尼指数 $Gini(D,A)$ 表示经 $A=a$ 分割后集合$D$的不确定性。<font><b>基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似</b></font>。\n",
    "\n",
    "**算法（CART生成算法）**\n",
    "\n",
    "输入： 训练数据集$D$\n",
    "\n",
    "输出：CART决策树\n",
    "\n",
    "根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：\n",
    "\n",
    "step1: 设结点的训练数据集为$D$, 计算现有特征对该数据集的基尼指数。此时，对每一个特征$A$，对其可能取的每个值$a$, 根据样本点对$A=a$的测试为\"是\"或\"否\" 将$D$分割成$D_1$和$D_2$两部分，利用公式\n",
    "#### $$Gini(D,A) = \\frac{|D1|}{|D|}Gini(D_1) + \\frac{|D2|}{|D|}Gini(D_2)$$\n",
    "计算$A=a$时的基尼指数。\n",
    "\n",
    "<font size=2 color='red'><b>*备注：类似于条件熵 H(D|A)*</b></font>\n",
    "\n",
    "step2: 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征和最优切分点。 依最优特征和最优切分点，从全网结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。\n",
    "\n",
    "<font size=2 color='red'><b>*备注：上面的关于特征A的 信息增益 $G(D,A) =H(D) - H(D|A)$ 选择信息增益最大的特征作为最优特征，由于$H(D)$ 对所有特征来说，都是一样的，因此问题转化为，求使条件熵 $H(D|A)$ 最小的特征*</b></font>\n",
    "\n",
    "step3: 对两个子结点递归地调用step1,step2, 直到满足停止条件。\n",
    "\n",
    "step4: 生成CART决策树。\n",
    "\n",
    "<font><b><i>算法停止的条件是结点中样本个数小于预定阈值，或者样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。</b></i></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## <font size=3><b>step2：决策树的剪枝</b></font>\n",
    "\n",
    "**算法(CART 剪枝算法)**\n",
    "\n",
    "输入： CART算法生成的决策树$T_0$\n",
    "\n",
    "输出： 最优决策树$T_a$\n",
    "\n",
    "step1: 设$k=0, T=T_0$\n",
    "\n",
    "step2: 设$\\alpha = +\\infty$\n",
    "\n",
    "step3: 自下而上地对各内部结点$t$ 计算$C(T_t), |T_t|$ 以及\n",
    "#### $$g(t) = \\frac{C(t) - C(T_t)}{|T_t| - 1}$$ $$\\alpha = min(\\alpha,g(t))$$\n",
    "这里，$T_t$表示以$t$为根结点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶结点个数。\n",
    "\n",
    "step4: 对$g(t) = \\alpha$ 的内部结点$t$进行剪枝，并对叶结点$t$以多数表决法决定其类，得到树$T$\n",
    "\n",
    "step5: 设$k=k+1, \\alpha_k = \\alpha, T_k = T$\n",
    "\n",
    "step6: 如果$T_k$不是由根结点及两个叶结点构成的树，则回到step2；否则，令$T_k = T_n$\n",
    "\n",
    "step7: 采用交叉验证法在子树序列$T_0,T_1,\\cdots,T_n$中选取最优子树$T_\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 习题5.1 根据申请人的特征利用决策树是否批准贷款申请"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dateset():\n",
    "    data = [['青年','否','否','一般','否'],\n",
    "            ['青年','否','否','好','否'],\n",
    "            ['青年','是','否','好','是'],\n",
    "            ['青年','是','是','一般','是'],\n",
    "            ['青年','否','否','一般','否'],\n",
    "            ['中年','否','否','一般','否'],\n",
    "            ['中年','否','否','好','否'],\n",
    "            ['中年','是','是','好','是'],\n",
    "            ['中年','否','是','非常好','是'],\n",
    "            ['中年','否','是','非常好','是'],\n",
    "            ['老年','否','是','非常好','是'],\n",
    "            ['老年','否','是','好','是'],\n",
    "            ['老年','是','否','好','是'],\n",
    "            ['老年','是','否','非常好','是'],\n",
    "            ['老年','否','否','一般','否']]\n",
    "    label=['年龄','有工作','有自己的房子','信贷情况','类别']\n",
    "    \n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>年龄</th>\n",
       "      <th>有工作</th>\n",
       "      <th>有自己的房子</th>\n",
       "      <th>信贷情况</th>\n",
       "      <th>类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>青年</td>\n",
       "      <td>否</td>\n",
       "      <td>否</td>\n",
       "      <td>一般</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>青年</td>\n",
       "      <td>否</td>\n",
       "      <td>否</td>\n",
       "      <td>好</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>青年</td>\n",
       "      <td>是</td>\n",
       "      <td>否</td>\n",
       "      <td>好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>青年</td>\n",
       "      <td>是</td>\n",
       "      <td>是</td>\n",
       "      <td>一般</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>青年</td>\n",
       "      <td>否</td>\n",
       "      <td>否</td>\n",
       "      <td>一般</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>中年</td>\n",
       "      <td>否</td>\n",
       "      <td>否</td>\n",
       "      <td>一般</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>中年</td>\n",
       "      <td>否</td>\n",
       "      <td>否</td>\n",
       "      <td>好</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中年</td>\n",
       "      <td>是</td>\n",
       "      <td>是</td>\n",
       "      <td>好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>中年</td>\n",
       "      <td>否</td>\n",
       "      <td>是</td>\n",
       "      <td>非常好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>中年</td>\n",
       "      <td>否</td>\n",
       "      <td>是</td>\n",
       "      <td>非常好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>老年</td>\n",
       "      <td>否</td>\n",
       "      <td>是</td>\n",
       "      <td>非常好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>老年</td>\n",
       "      <td>否</td>\n",
       "      <td>是</td>\n",
       "      <td>好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>老年</td>\n",
       "      <td>是</td>\n",
       "      <td>否</td>\n",
       "      <td>好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>老年</td>\n",
       "      <td>是</td>\n",
       "      <td>否</td>\n",
       "      <td>非常好</td>\n",
       "      <td>是</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>老年</td>\n",
       "      <td>否</td>\n",
       "      <td>否</td>\n",
       "      <td>一般</td>\n",
       "      <td>否</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    年龄 有工作 有自己的房子 信贷情况 类别\n",
       "0   青年   否      否   一般  否\n",
       "1   青年   否      否    好  否\n",
       "2   青年   是      否    好  是\n",
       "3   青年   是      是   一般  是\n",
       "4   青年   否      否   一般  否\n",
       "5   中年   否      否   一般  否\n",
       "6   中年   否      否    好  否\n",
       "7   中年   是      是    好  是\n",
       "8   中年   否      是  非常好  是\n",
       "9   中年   否      是  非常好  是\n",
       "10  老年   否      是  非常好  是\n",
       "11  老年   否      是    好  是\n",
       "12  老年   是      否    好  是\n",
       "13  老年   是      否  非常好  是\n",
       "14  老年   否      否   一般  否"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "data,label = get_dateset()\n",
    "\n",
    "datasets = pd.DataFrame(data,columns=label)\n",
    "\n",
    "datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________\n",
    "<font color='red' size=3><b>step1: 计算数据集$D$的经验熵$H(D)$</b></font>\n",
    "#### $$H(D) = -\\sum_{k=1}^K\\frac{|C_k|}{D}log_2\\frac{|C_k|}{D}$$\n",
    "__________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ent(datasets):\n",
    "    '''\n",
    "    输入：数据集D\n",
    "    输出：数据集D的经验熵H(D)\n",
    "    '''\n",
    "    \n",
    "    label_set = Counter(datasets.iloc[:,-1])\n",
    "    p = np.array(list(label_set.values()))/len(datasets)\n",
    "    ent = -sum(p*np.log2(p))\n",
    "\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________\n",
    "<font color='red' size=3><b>step2: 计算特征$A$对数据集$D$的经验熵$H(D|A)$ </b></font>   \n",
    "   ### $$H(D|A) = \\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i) = -\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}^K\\frac{|D_{ik}|}{|D_i|}log_2\\frac{|D_{ik}|}{|D_i|}$$\n",
    "__________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_condi_ent(datasets,column_name):\n",
    "    \"\"\"\n",
    "    计算条件熵\n",
    "    输入：数据集，列名\n",
    "    输出：条件熵P(D|A)\n",
    "    \"\"\"\n",
    "    feature_set = Counter(datasets[column_name])\n",
    "    feature_ent_set={}\n",
    "\n",
    "    for key in feature_set.keys():\n",
    "        # print(\"key:\",key,\"value\",feature_set[key])\n",
    "        sub_dataset = datasets.loc[datasets[column_name]==key]\n",
    "        # print(sub_dataset)\n",
    "        sub_ent = cal_ent(sub_dataset)\n",
    "        # print(\"sub_ent:\",sub_ent)\n",
    "        feature_ent_set[key] = [feature_set[key], sub_ent]\n",
    "\n",
    "    ent = sum([i[0] / len(datasets)*i[1] for i in feature_ent_set.values()])\n",
    "\n",
    "    return ent  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________\n",
    "<font color='red' size=3><b>step3: 计算信息增益</b></font> \n",
    " ### $$g(D,A) = H(D) - H(D|A)$$\n",
    " __________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gain(ent,condi_ent):\n",
    "    return ent - condi_ent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取最大的信息增益的特征作为节点\n",
    "def get_node(datasets,features):\n",
    "    \"\"\"\n",
    "    返回信息增益最大的特征\n",
    "    \"\"\"\n",
    "    feature_gain = []\n",
    "    for i in range(len(features)):\n",
    "        column_name = features[i]\n",
    "        ent = cal_ent(datasets)\n",
    "        condi_ent = cal_condi_ent(datasets,column_name)\n",
    "\n",
    "        feature_gain.append((column_name,cal_gain(ent,condi_ent)))\n",
    "\n",
    "    max_gain_node_index=feature_gain.index(max(feature_gain,key = lambda x:x[1]))\n",
    "    \n",
    "    max_feature = max_gain_node_index # 最大信息熵对应特征\n",
    "    max_feature_name = feature_gain[max_gain_node_index][0] # 最大信息熵对应特征名称\n",
    "    max_feature_gain = feature_gain[max_gain_node_index][1] # 最大信息熵\n",
    "\n",
    "    return (max_feature,max_feature_name,max_feature_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font><b>——————————————————————————————————————————————————————————————————————————————————</b></font>\n",
    "<font size =4 color='blue'><b>利用ID3算法构建决策树</b></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算法（ID3算法）**\n",
    " \n",
    " 输入：训练数据集$D$，特征集$A$阈值$\\epsilon$\n",
    " \n",
    " 输出：决策树T\n",
    " \n",
    " step1: 若$D$中所有实例属于同一类$C_k$，则$T$为单结点树，并将类$C_k$作为该结点的类标记，返回$T$\n",
    " \n",
    " step2: 若$A = \\emptyset$ , 则$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$\n",
    " \n",
    " step3: 否则，按算法5.1计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_g$\n",
    " \n",
    " step4: 如果$A_g$的信息增益小于阈值$\\epsilon$，则$T$为单结点树，并将$D$中实例最大的类$C_k$作为该结果的类标记，返回$T$\n",
    " \n",
    " step5：否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及子结点构成树$T$，返回$T$\n",
    " \n",
    " step6: 对第$i$个子结点，以$D_i$为训练集，以$A - \\{A_g\\}$为特征集，递归地调用step1-step5，得到子树$T_i$，返回$T_i$\n",
    " \n",
    "**算法（ID3算法）** 用信息增益比来选择特征，其他仍然与ID3一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ NEW \n",
    "epsilion = 0.1  # 信息增益的阈值\n",
    "\n",
    "# node = {\"root\":False,label\":None,\"feature\":None,\"feature_name\":None,tree\":{}}\n",
    "\n",
    "node = {\"root\":False,\"label\":None,\"feature\":None,\"feature_name\":None,\"tree\":{}}\n",
    "\n",
    "def train_node(datasets,epsilion=0.1):\n",
    "    \n",
    "    \n",
    "    X_train = datasets.iloc[:,:-1]\n",
    "    y_train = datasets.iloc[:,-1]\n",
    "    features = datasets.columns[:-1]\n",
    "\n",
    "    end_Flag = False \n",
    "    \n",
    "    # step1:若 D中所有实例属于同一类Ck，则T为单结点树，并将类Ck作为该结点的类标记，返回T\n",
    "    if len(y_train.value_counts())==1:\n",
    "        \n",
    "        node[\"label\"] = datasets.iloc[0,-1]\n",
    "        node[\"root\"] = True\n",
    "        \n",
    "        print(\"step1:\", node)\n",
    "        \n",
    "        end_Flag = True\n",
    "        return end_Flag\n",
    "\n",
    "    # step2: 若 A为空，则T为单结点树，并将 D 中实例数最大的类 Ck作为该结点的类标记，返回 T\n",
    "    if len(features)==0:\n",
    "\n",
    "        node[\"label\"] = y_train.value_counts().sort_values(ascending=False).index[0]\n",
    "        node[\"root\"] = True\n",
    "        print(\"step2:\",node)\n",
    "        \n",
    "        end_Flag = True\n",
    "        return end_Flag\n",
    "\n",
    "    # step3: 否则，按算法5.1计算 A 中各特征对 D的信息增益，选择信息增益最大的特征Ag\n",
    "    (max_feature,max_feature_name,max_feature_gain) = get_node(datasets,features)\n",
    "    print(\"step3:\",(max_feature,max_feature_name,max_feature_gain))\n",
    "\n",
    "\n",
    "    # step4: 如果 Ag 的信息增益小于阈值epsilon，则T为单结点树，并将D中实例最大的类C_k作为该结果的类标记，返回T\n",
    "    if max_feature_gain < epsilion : \n",
    "        \n",
    "        node[\"label\"] = y_train.value_counts().sort_values(ascending=False).index[0]\n",
    "        node[\"root\"] = True\n",
    "        print(\"step4:\",node)\n",
    "        \n",
    "        end_Flag = True\n",
    "        return end_Flag\n",
    "    \n",
    "\n",
    "    # step5: 否则，对 Ag 的每一可能值ai，依Ag = ai将 D 分割为若干非空子集 Di，将Di中实例数最大的类作为标记，构建子结点，由结点及子结点构成树 T，返回 T\n",
    "\n",
    "    node[\"feature\"] = max_feature\n",
    "    node[\"feature_name\"] = max_feature_name \n",
    "    print(\"step5:\",node)\n",
    "    \n",
    "    end_Flag = False \n",
    "    \n",
    "    return end_Flag,max_feature,max_feature_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: Index(['年龄', '有工作', '有自己的房子', '信贷情况'], dtype='object')\n",
      "    年龄 有工作 有自己的房子 信贷情况 类别\n",
      "0   青年   否      否   一般  否\n",
      "1   青年   否      否    好  否\n",
      "2   青年   是      否    好  是\n",
      "3   青年   是      是   一般  是\n",
      "4   青年   否      否   一般  否\n",
      "5   中年   否      否   一般  否\n",
      "6   中年   否      否    好  否\n",
      "7   中年   是      是    好  是\n",
      "8   中年   否      是  非常好  是\n",
      "9   中年   否      是  非常好  是\n",
      "10  老年   否      是  非常好  是\n",
      "11  老年   否      是    好  是\n",
      "12  老年   是      否    好  是\n",
      "13  老年   是      否  非常好  是\n",
      "14  老年   否      否   一般  否\n",
      "step3: (2, '有自己的房子', 0.4199730940219749)\n",
      "step5: {'root': False, 'label': None, 'feature': 2, 'feature_name': '有自己的房子', 'tree': {}}\n",
      "(False, 2, '有自己的房子')\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(train_node(datasets,epsilion=0.1))\n",
    "print(node[\"tree\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step3: (2, '有自己的房子', 0.4199730940219749)\n",
      "step5: {'root': False, 'label': None, 'feature': 2, 'feature_name': '有自己的房子', 'tree': {}}\n",
      "feature_list: Index(['否', '是'], dtype='object')\n",
      "step3: (1, '有工作', 0.9182958340544896)\n",
      "step5: {'root': False, 'label': None, 'feature': 1, 'feature_name': '有工作', 'tree': {}}\n",
      "step1: {'root': True, 'label': '是', 'feature': 1, 'feature_name': '有工作', 'tree': {'否': (False, 1, '有工作')}}\n"
     ]
    }
   ],
   "source": [
    "eplision = 0.1\n",
    "\n",
    "(end_flag,max_feature,max_feature_name) = train_node(datasets,epsilion)\n",
    "\n",
    "if end_flag == False :\n",
    "    feature_list = datasets[max_feature_name].value_counts().index\n",
    "    print(\"feature_list:\",feature_list)\n",
    "\n",
    "    for f in feature_list:\n",
    "        sub_train_dataset = datasets.loc[datasets[max_feature_name] == f].drop([max_feature_name], axis =1)\n",
    "    \n",
    "        # step6： 递归生成树\n",
    "        sub_tree = train_node(sub_train_dataset,eplision)\n",
    "        node['tree'][f] = sub_tree\n",
    "\n",
    "# return node_tree\n",
    "# train(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
