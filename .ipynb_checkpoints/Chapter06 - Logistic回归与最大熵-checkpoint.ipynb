{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font><b>第六章 Logistic回归与最大熵模型</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型(maxim entropy model)\n",
    "\n",
    "2、Logistic回归模型与最大熵模型都属于对数线性模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'><b>一、Logistic回归</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = 4><b>Logistic分布</b></font>\n",
    "\n",
    "**定义（Logistic分布）** 设$X$是连续随机变量，$X$服从Logistic分布是指$X$具有下列分布函数和密度函数：\n",
    "#### $$F(x) = P(X\\leq x) = \\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$$ $$f(x) = F'(x) = \\frac{e^{-(x-\\mu)/\\gamma}}{\\gamma(1+e^{-(x-\\mu)/\\gamma})^2}$$\n",
    "其中，$\\mu$为位置参数， $\\gamma >0$为形状参数\n",
    "\n",
    "* <font size = 4><b>二项Logistic回归模型(binomial logistic regression mmodel)</b></font> 是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的logistic分布。这里，随机变量$X$取值为实数，随机变量$Y$取值为1或0.\n",
    "\n",
    "**定义（Logistic回归模型）** 二项Logistic回归模型是如下的条件概率分布：\n",
    "#### $$P(Y=1|x) = \\frac{exp(w\\cdot x +b)}{1 + exp(w\\cdot x +b)}\\\\ P(Y=0|x) = \\frac{1}{1 + exp(w\\cdot x +b)}$$ \n",
    "\n",
    "其中，$x\\in R^n$是输入，$Y\\in\\{0,1\\}$是输出，$w$ 为权值向量，$b$偏置，$w\\cdot b$ 为$w$ 和$b$的内积\n",
    "\n",
    "\n",
    "**Logistic回归模型的特点：** 一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值，即\n",
    "#### $$\\frac{p}{1-p}$$\n",
    ", 其中，$p$为该事件发生的概率。该事件的对数几率(log odds)或logit函数是 \n",
    "#### $$\\begin{align} & logit(p) = log\\frac{p}{1-p} \\\\ \n",
    "         \\Rightarrow & logit(p) = log\\frac{P(Y=1|x)}{P(Y=0|x)} = log\\frac{\\frac{exp(w\\cdot x +b)}{1 + exp(w\\cdot x +b)}}{\\frac{1}{1 + exp(w\\cdot x +b)}}\\\\\n",
    "         \\Rightarrow & logit(p) = log(exp(w\\cdot x+b)) = w\\cdot x + b  \\end{align}$$\n",
    "\n",
    "\n",
    "* <font size = 4><b>模型参数估计</b></font> Logistic回归模型学习时，对于给定的训练数据集$T = \\{(x_i,y_i)\\},i = 1,2,\\cdots,N; y_i \\in \\{0,1\\}$, 可以应用 **极大似然估计法**估计模型参数：\n",
    "\n",
    "设：\n",
    "#### $$P(Y=1|x) = \\pi(x), P(Y=0|x) = 1 - \\pi(x)$$\n",
    "似然函数为\n",
    "### $$\\prod_{i=1}^N[\\pi(x_i)]^{y_i}[1 - \\pi(x_i)]^{1 - y_i}$$\n",
    "对数似然函数为 \n",
    "#### $$\\begin{align} L(w) &= \\sum_{i=1}^N[y_ilog\\pi(x_i) + (1-y_i)log(1-\\pi(x_i))] \\\\ \n",
    "                               &= \\sum_{i=1}^N[y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)} + log(1-\\pi(x_i))] \\\\\n",
    "                               &= \\sum_{i=1}^N[y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)} + log(1-\\frac{exp(w\\cdot x_i)}{1+exp(w\\cdot x_i)})] \\\\   \n",
    "                               &= \\sum_{i=1}^N[y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)} + log(\\frac{1}{1+exp(w\\cdot x_i)})] \\\\  \n",
    "                               & = \\sum_{i=1}^N[y_i(w\\cdot x_i) - log(1+exp(w\\cdot x_i))] \\end{align}$$\n",
    "对$L(w)$求极大值，得到$w$的估计值\n",
    "\n",
    "假设$w$的极大似然估计值是$\\hat{w}$, 那么学得的Logistic回归模型是\n",
    "#### $$P(Y=1|x) = \\frac{exp(\\hat{w}\\cdot x)}{1+exp(\\hat{w}\\cdot x)} \\\\ P(Y=0|x) = \\frac{1}{1+exp(\\hat{w}\\cdot x)}$$\n",
    "\n",
    "\n",
    "* <font size = 4><b>多项Logistic回归（multi-nominal logistic regression model）</b></font> 用于多分类。假设离散随机变量$Y$的取值集合是$\\{1,2,\\cdots,K\\}$，那么多项式Logistic回归模型是\n",
    "#### $$P(Y=k|x) = \\frac{exp(w_k\\cdot x)}{1+\\sum\\limits_{k=1}^{K-1}exp(w_k\\cdot x) }, k=1,2,\\cdots,K-1$$\n",
    "#### $$P(Y=K|x) = \\frac{1}{1+\\sum\\limits_{k=1}^{K-1}exp(w_k\\cdot x) }, k=1,2,\\cdots,K-1$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'><b>二、最大熵模型</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = 4><b>最大熵原理</b></font> 最大熵原理是概率模型学习的一个准则。\n",
    "\n",
    "假设离散随机变量$X$的概率分布是$P(X)$，则其熵是\n",
    "#### $$H(P) = -\\sum_xP(x)logP(x)$$\n",
    "熵满足下列不等式：\n",
    "#### $$0 \\leq H(P) \\leq log|X|$$\n",
    "式中，$|X|$是$X$的取值个数。当且仅当$X$的分布是均匀分布时，右边的等号成立。也就是说，当$X$服从均匀分布时，熵最大。\n",
    "\n",
    "\n",
    "* <font size = 4><b>最大熵模型定义</b></font> 假设分类模型是一个条件概率分布$P(Y|X)$\n",
    "\n",
    "* 给定训练数据集，其\n",
    "### 联合分布 $\\tilde{P}(X =x,Y=y) = \\frac{\\nu(X =x,Y=y)}{N}$\n",
    "### 边缘分布 $\\tilde{P}(X =x) = \\frac{\\nu(X =x)}{N}$ \n",
    "其中，$\\nu(X =x,Y=y)$ 表示训练数据中，样本$(x,y)$出现的频数，$\\nu(X =x)$ 表示输出$X$出现的频数，$N$表示训练样本数量\n",
    "\n",
    "用特征函数$f(x,y)$描述输入$x$和输出$y$之间的某一事件：\n",
    "#### $$f(x,y) = \\left\\{\\begin{aligned} 1         ,& x与y满足某一事实 \\\\\n",
    "                                       0         ,& otherwise\n",
    "                                       \\end{aligned}\\right.$$\n",
    "                                       \n",
    "特征函数$f(x,y)$关于经验分布$\\tilde{P}(X,Y)$的期望：<font size=4><b>$E_{\\tilde{P}}(f) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y)$</b></font>\n",
    "\n",
    "特征函数$f(x,y)$关于模型$P(X,Y)$与经验分布$\\tilde{P}(X)$的期望：<font size=4><b>$E_{P}(f) = \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)$</b></font>\n",
    "\n",
    "假设这两个期望相等（约束条件）：\n",
    "#### $$\\begin{align} & E_{P}(f) = E_{\\tilde{P}}(f) \\\\ \\Rightarrow & \\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y) = \\sum_{x,y}\\tilde{P}(x,y)f(x,y) \\end{align} $$\n",
    "\n",
    "#### **定义（最大熵模型）** \n",
    "\n",
    "假设满足所有约束条件的模型集合为\n",
    "#### $$C \\equiv -\\sum_{x,y}\\tilde{P}(x)P(y|x)logP(y|x)$$\n",
    "则模型集合$C$中条件熵$H(P)$最大的模型称为最大熵模型。式中的对数为自然对数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = 4><b>最大熵模型的学习</b></font> 最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优化问题\n",
    "\n",
    "对于给定的训练数据集$T=\\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$，及特征函数$f(x,y), i=1,2,\\cdots,n$, 最大熵模型的学习等价于约束最优化问题：\n",
    "####  $$\\begin{align} \\mathop {max}\\limits_{P\\in C} & H(P) = - \\sum_{x,y}\\tilde{P}(x)P(y|x)logP(y|x)  \\\\\n",
    "                                               s.t.  \\quad & E_{P}(f_i) = E_{\\tilde{P}}(f_i), i=1,2,\\cdots,n \\\\\n",
    "                                                    & \\sum_y P(y|x) =1 \\end{align}\n",
    "$$\n",
    "按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题：\n",
    "####  $$\\begin{align} \\mathop {min}\\limits_{P\\in C} & -H(P) = \\sum_{x,y}\\tilde{P}(x)P(y|x)logP(y|x)  \\\\\n",
    "                                        s.t.  \\quad & E_{P}(f_i) - E_{\\tilde{P}}(f_i) = 0 , i=1,2,\\cdots,n \\\\\n",
    "                                                    & \\sum_y P(y|x) =1 \\end{align}\n",
    "$$\n",
    "**解：**\n",
    "1）引入拉格朗日乘子$w_0,w_1,\\cdots,w_n$， 定义拉格朗日函数$L(P,w)$: \n",
    "#### $$\\begin{align} L(P,w) & \\equiv -H(P) + w_0(1-\\sum_y P(y|x)) + \\sum_{i=1}^nw_i(E_{\\tilde{P}}(f_i) - E_{P}(f_i)) \\\\\n",
    "                            & = \\sum_{x,y}\\tilde{P}(x)P(y|x)logP(y|x) + w_0(1-\\sum_y P(y|x)) + \\sum_{i=1}^nw_i(\\sum_{x,y}\\tilde{P}(x,y)f_i(x,y) - \\sum_{x,y}\\tilde{P}(x)P(y|x)f_i(x,y)) \\end{align}$$\n",
    "\n",
    "\n",
    "*****\n",
    "\n",
    "\n",
    "* <font size = 4><b>极大似然估计</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最优化的原始问题是\n",
    "#### $$\\mathop {min}\\limits_{P\\in C} \\mathop{max}\\limits_w L(P,w)$$ \n",
    "其对偶问题是\n",
    "#### $$\\mathop{max}\\limits_w  \\mathop {min}\\limits_{P\\in C} L(P,w)$$ \n",
    "由于原始问题与对偶问题是等价的，可以通过求解对偶问题来求解原始问题。\n",
    "\n",
    "设关于$w$的函数 $\\mathop {min}\\limits_{P\\in C} L(P,w)$\n",
    "#### $$\\Psi (w) = \\mathop {min}\\limits_{P\\in C} L(P,w) = L(P_w,w)$$\n",
    "其中，\n",
    "#### $$P_w = arg \\mathop{min}\\limits_{P\\in C}L(P,w) = P_w(y|x)$$\n",
    "具体的，求$L(P,w)$对$P(y|x)$的偏导数\n",
    "#### $$\\begin{align} \\frac{\\partial{L(P,w)}}{\\partial{P(y|x)}} &= \\sum_{x,y}\\tilde{P}(x)logP(y|x) + \\sum_{x,y}\\tilde{P}(x)P(y|x)\\frac{1}{P(y|x)} - \\sum_yw_0 + \\sum_{i=1}^nw_i(0 - \\sum_{x,y}\\tilde{P}(x)f_i(x,y)) \\\\\n",
    "                                                               &= \\sum_{x,y}\\tilde{P}(x)(logP(y|x) + 1) - \\sum_yw_0 - \\sum_{x,y}(\\tilde{P}(x)\\sum_{i=1}^nw_if_i(x,y)) \\\\\n",
    "                                                               &= \\sum_{x,y}\\tilde{P}(x)\\big(logP(y|x) + 1 - w_0 - \\sum_{i=1}^nw_if_i(x,y)\\big)\n",
    "\\end{align}$$\n",
    "\n",
    "#### $$令 \\quad \\frac{\\partial{L(P,w)}}{\\partial{P(y|x)}} =0 \\quad \\Rightarrow P(y|x) = exp\\big( \\sum_{i=1}^nw_if_i(x,y) + w_0 -1 \\big) = \\frac{exp\\big( \\sum\\limits_{i=1}^nw_if_i(x,y) \\big)}{exp(1-w_0)}$$\n",
    "\n",
    "\n",
    "#### $由于 \\sum_yP(y|x) = 1 $ ： \n",
    "\n",
    "最大熵模型\n",
    "#### $$P_w = P_w(y|x) = \\frac{1}{Z_w(x)} exp\\big(\\sum_{i=1}^n w_if_i(x,y) \\big), 其中，Z_w(x) = \\sum_y exp\\big( \\sum_{i=1}^n w_if_i(x,y)\\big)$$\n",
    "其中，$Z_w(x)$称为规范化因子。$f_i(x,y)$是特征函数，$w_i$是特征的权值。$P_w中的w$是最大熵模型中的参数向量。\n",
    "\n",
    "最后，求解 \n",
    "#### $$w^{*} = arg \\mathop {max}\\limits_w \\Psi (w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'><b>三、模型学习的最优化算法</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = 4><b>改进的迭代尺度法（improved iterative scaling,IIS）</b></font> 是一种最大熵模型学习的最优化算法。\n",
    "<font size = 8> \n",
    "\n",
    "**算法（改进的迭代尺度法（improved iterative scaling,IIS））**\n",
    "    \n",
    "输入： 特征函数$f_1,f_2,\\cdots,f_n$；经验分布$\\tilde{P}(X,Y)$, 模型$P_w(y|x)$ \n",
    "\n",
    "输出：最优参数值 $w_i^*$  ; 最优模型$P_{w^*}$\n",
    "    \n",
    "(1) 对所有$i\\in\\{1,2,\\cdots,n\\}$, 取初始$w_i=0$\n",
    "\n",
    "(2) 对每一个 $i\\in\\{1,2,\\cdots,n\\}$ \n",
    "\n",
    "   (a) 令$\\delta_i$ 是方程\n",
    "   ### $$\\sum_{x,y}\\tilde{P}(x)P(y|x)f_i(x,y)exp(\\delta_i f^\\#(x,y)) = E_{\\tilde{P}}(f_i)$$\n",
    "   的解 ，其中，\n",
    "   #### $$f^\\#(x,y) = \\sum_{i=1}^nf_i(x,y)$$\n",
    "   \n",
    "   (b) 更新$w_i$的值： $w_i\\leftarrow w_i+\\delta_i$\n",
    "   \n",
    "   (3) 如果不是所有$w_i$都收敛，重复(2)\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size = 4><b>拟牛顿法（BFGS） </b></font>\n",
    "\n",
    "**算法(最大熵模型学习的BFGS算法)** \n",
    "\n",
    "输入：特征函数$f_1,f_2,\\cdots,f_n$；经验分布$\\tilde{P}(X,Y)$,目标函数$f(w)$，梯度$g(w) = \\nabla f(w)$, 精度要求 $\\epsilon$\n",
    "\n",
    "输出：最优参数值$w^*$,最优模型P_w（y|x)\n",
    "\n",
    "(1) 选定初始点$w^{(0)}$,取$B_0$为正定对称矩阵，置k=0\n",
    "(2) 计算$g(k) = g(w^{(k)})$。 若$||g_k||<\\epsilon$，则停止计算，得$w^* = w^{(k)}$,否则转（3）\n",
    "(3) 由$B_kp_k = - g_k$ 求出 $p_k$\n",
    "(4) 一维搜索：求$\\lambda_k$ 使得\n",
    "#### $$f(w^{(k)} + \\lambda_kp_k) = \\mathop {min}\\limits_{\\lambda \\geq 0}f(w^{(k)} + \\lambda p_k)$$\n",
    "\n",
    "(5) 置<font size=4>$w^{(k+1)} = w^{(k)} + \\lambda_kp_k$</font>\n",
    "\n",
    "(6) 计算$g_{k+1} = g(w^{(k+1)})$ 若$||g_{k+1}||<\\epsilon$, 则停止计算，得$w^* = w^{(k+1)}$; 否则按下式求出$B_{k+1}$: \n",
    "#### $$B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T\\delta_k} - \\frac{B_k\\delta_k\\delta_k^TB_k}{\\delta_k^TB_k\\delta_k}$$\n",
    "其中，\n",
    "#### $$y_k = g_{k+1} - g_k, \\delta_k = w^{(k+1)} -  w^{(k)}$$\n",
    "\n",
    "(7) 置k=k+1,转(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "def get_data():\n",
    "    # 加载数据\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "    df[\"label\"] = iris.target\n",
    "\n",
    "    # 重新定义列名\n",
    "    df.columns = [\"sepal length\",\"sepal width\",\"petal length\",\"petal width\",\"label\"]\n",
    "\n",
    "    # 定义输入数据集 取第0，1，及最后一列，前100条数据\n",
    "    data = np.array(df.iloc[:100,[0,1,-1]])\n",
    "\n",
    "    X=data[:,:-1]\n",
    "    y=data[:,-1]\n",
    "\n",
    "    return X,y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_data()\n",
    "\n",
    "# 划分训练集、测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 200 #最大迭代次数\n",
    "learning_rate = 0.01\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def data_matrix(X):\n",
    "    data_mat = []\n",
    "    \n",
    "    for d in X:\n",
    "        data_mat.append([1,*d])\n",
    "        \n",
    "    return data_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logisticRegression Model(learning_rate = 0.01,max_iter=200)\n",
      "weights: [[-1.1407939]\n",
      " [ 3.8921633]\n",
      " [-6.2533207]]\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "data_mat = data_matrix(X)\n",
    "\n",
    "weights = np.zeros(((len(data_mat[0]),1)),dtype = np.float32)\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    for i in range(len(X)):\n",
    "        result = sigmoid(np.dot(data_mat[i],weights))\n",
    "        err = y[i] - result \n",
    "        weights += learning_rate*err*np.transpose([data_mat[i]])\n",
    "\n",
    "print(\"logisticRegression Model(learning_rate = {},max_iter={})\".format(learning_rate,max_iter))\n",
    "print(\"weights:\",weights)\n",
    "# return -(weights[0] + weights[1] * X) / weights[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d216cffa58>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU9dn/8fdNEkjYZVMhCQGCVAUUQRFZZNGHSnGtVawbxUrdiq2tfWqfPm2lfX7V2qs1iEvRttalVLS4VrElIQqKYAAFBSVhTcIeTNgSyHL//piJ4pjlJDmTc+bM/bouLjJnDme+3xn5eDjnk29EVTHGGBP72ng9AGOMMe6wQDfGmICwQDfGmICwQDfGmICwQDfGmIBI9OqFe/TooRkZGV69vDHGxKRVq1btU9WedT3nONBFJAHIA4pVdWrEc9OBB4Di8Ka5qvpEQ8fLyMggLy/P6csbY4wBRGRbfc815Qz9TmAD0Lme559T1TuaMjBjjDHucXQNXURSgW8ADZ51G2OM8Y7Tm6IPAj8BahrY55sislZEXhCRtLp2EJGZIpInInl79+5t6liNMcY0oNFLLiIyFdijqqtEZHw9u70KzFfVoyJyC/A3YGLkTqo6D5gHMGLEiK+sOVBZWUlRUREVFRVNmELrS05OJjU1laSkJK+HYowxn3NyDX00cImITAGSgc4i8oyqXle7g6qWHLf/48D9zRlMUVERnTp1IiMjAxFpziGiTlUpKSmhqKiIfv36eT0cY4z5XKOXXFT1HlVNVdUMYBqQc3yYA4jIycc9vITQzdMmq6iooHv37r4NcwARoXv37r7/V4QxJv40u4cuIrOBPFV9BZglIpcAVcB+YHoLjtvcP9pqYmGMxpj406RAV9VcIDf89S+O234PcI+bAzPGGNM09q3/dVi0aBGDBg0iMzOT++67z+vhmBj00ppiRt+XQ7+f/ovR9+Xw0prixv+QMS1kgR6hurqa22+/nTfeeIP169czf/581q9f7/WwTAx5aU0x9yxcR3FpOQoUl5Zzz8J1Fuom6mI60KNxFrRy5UoyMzPp378/bdu2Zdq0abz88ssujNbEiwfe/JTyyuovbSuvrOaBNz/1aEQmXsRsoEfrLKi4uJi0tC++Lyo1NZXiYjuzMs7tKC1v0nZj3BKzgR6ts6C6fsaqtVpMU/TumtKk7ca4JWYDPVpnQampqRQWFn7+uKioiN69e7fomCa+3D15EClJCV/alpKUwN2TB3k0IhMvYjbQo3UWdPbZZ5Ofn8+WLVs4duwY//jHP7jkkktadEwTXy4b1offXjGEPl1TEKBP1xR+e8UQLhvWx+uhmYDz7AdctNTdkwdxz8J1X7rs4sZZUGJiInPnzmXy5MlUV1czY8YMTj/99JYO18SZy4b1sQA3rS5mA732L8sDb37KjtJyendN4e7Jg1z5SzRlyhSmTJnS4uMYY0xritlABzsLMsaY48XsNXRjjDFfZoFujDEBYYFujDEBYYFujDEBYYFujDEBYYEeYcaMGfTq1YvBgwd7PRRjjGkSC/QI06dPZ9GiRV4Pw7QyW7/cBEFsB/raBfDHwfCrrqHf1y5o8SHHjRtHt27dXBiciRW2frkJitgN9LUL4NVZUFYIaOj3V2e5Euomvtj65SYoYjfQs2dDZcTKipXloe3GNIGtX26ibc/BCvYdOhr114ndQC8ratp2Y+ph65ebaNlzsIJfv7aecb9bQtbi/Ki/XuwGepfUpm03ph62frlx254DFcx+dT1j71/Ck+9u5RtDejNjTL+ov27sLs416Reha+bHX3ZJSgltb4FrrrmG3Nxc9u3bR2pqKvfeey833XRTCwdr/CyaK3ea+LL7QAWP5m5i/srtVNUolw/rwx0TMsno0aFVXj92A33oVaHfs2eHLrN0SQ2Fee32Zpo/f74LgzOxxlbuNC2xq6yCx97axN9Xbqe6RrliWB/umJhJ3+6tE+S1YjfQIRTeLQxw4y8vrSm2M2UTM3aVVfBobgHz3y+kpkb55lmp3D4hk/Tu7T0ZT2wHugmU2j54bYWwtg8OWKgbX9lZVs6juZv4x8pCalS5cngoyNO6eRPktXwX6KqKiHg9jAapqtdDCKSG+uAW6MYPdpSW80huAQveL6JGlW+NSOW28d4HeS1fBXpycjIlJSV0797dt6GuqpSUlJCcnOz1UALH+uDGr4pLy3lkSQEL8goBuHJ4GreNH+CbIK/lq0BPTU2lqKiIvXv3ej2UBiUnJ5OaavVIt/XumkJxHeFtfXDjlaLPjvBI7iaeDwf5t0aEgjz1BH8FeS1fBXpSUhL9+kW/q2n86e7Jg750DR2sD268UfTZER5esokXVoWC/Oqz07h1fCZ9fH5y4atAN/HN+uDGa4X7j/BIbgHP5xXRRoRpZ6dz6/gBMfOvRMeBLiIJQB5QrKpTI55rBzwFDAdKgKtVdauL4zRxwvrgxguF+48wN6eAf64OBfm3R4aC/OQusRHktZpyhn4nsAHoXMdzNwGfqWqmiEwD7geudmF8xgSa9e69tb3kCHOX5LNwdTFt2gjXjkznlhgM8lqOAl1EUoFvAP8H3FXHLpcCvwp//QIwV0RErd9nTL2sd++dbSWHmZtTwMI1xSS0Ea47ty+3nD+Ak7rEdnvN6Rn6g8BPgE71PN8HKARQ1SoRKQO6A/taPEJjAsp6961v677DzF1SwItriklsI9wwKhTkJ3aO7SCv1Wigi8hUYI+qrhKR8fXtVse2r5ydi8hMYCZAenp6E4ZpTPBY7771bNl3mIdy8nn5gx0kthFuHJXBLef3p1dAgryWkzP00cAlIjIFSAY6i8gzqnrdcfsUAWlAkYgkAl2A/ZEHUtV5wDyAESNG2OUYE9esdx99m/ceYm5OAS99UEzbxDZMPy+D753fn16dghXktRoNdFW9B7gHIHyG/uOIMAd4BbgRWA5cCeTY9XNjGma9++jZFA7yl8NBPmN0P2YGOMhrNbuHLiKzgTxVfQX4M/C0iBQQOjOf5tL4jAks6927r2DPIebm5PPKhztom9iG747tz81j+9OzUzuvh9YqxKsT6REjRmheXp4nr22MCZaCPQeZk13Aq2t3kJyYwA2j+nLzuP706Bi8IBeRVao6oq7n7DtFTaD9/KV1zF9RSLUqCSJcMzKN31w2xOthGZfk7z7InJwCXlu7g5SkBGaO68/Msf3pHsAgd8IC3QTWz19axzPvbf/8cbXq548t1GPbxt0HmZOdz7/W7SQlKYHvjRvAzWP7xW2Q17JAN4E1f0Vhvdst0GPTp7sOMicnn9fX7aR9UgK3nj+A747tT7cObb0emi9YoJvAqq7n/lB9241/fbLrAHOy83l93S46tE3gtvED+O6Y/pxgQf4lFugmsBJE6gzvBJ/+8BTzVRt2hoL8jY920bFdIndMyOSmMf0syOthgW4C65qRaV+6hn78duNv63eEgnzRx7vo1C6R708MBXnX9hbkDbFAN4FVe53cWi6x4+MdZczJzufNj3fTqV0isyYN5KbR/ejSPsnrocUE66EbYzz3UXEoyP+9fjedkhOZMbofMyzI62Q9dOO5ax9fzjubvljeZ/SAbjx78ygPRxRdts65Mx8Vl/Hg4nwWbwgF+Q8uGMh3RvejS4oFeXNYoJuoiwxzgHc27efax5cHMtRtnfPGrSsqIyt7I4s37KFzciI/vOAUpo/OsCBvIQt0E3WRYd7Y9lhn65zXb21RKVmL88n+ZA9dUpL40YWncOPoDDonW5C7wQLdGJfZOudf9WFhKVnZ+eR8soeu7ZP48X+dwo3nZdDJgtxVFujGuMzWOf/Cmu2fkZWdT+6ne+naPom7Jw/ihlF9LcijxALdRN3oAd3qvLwyekA3D0YTfbbOOaze/hlZi/N5a+NeTggH+Y3nZdCxnUVONNm7a6Lu2ZtHxVXLJZ7XOV+1LXRG/nY4yP/761/j+lF9LchbifXQjTEttmrbfh5cnM/S/H1069CWmeP6c/25felgQe4666Ebz7ndy3Z6POuDR1fe1lCQLyvYR/cObbnnoq9xnQW5Z+xdN1Hndi/b6fGsDx49K7fsJyt7I+8UlNCjY1t+NiUU5O3bWqR4yd59E3Vu97KdHs/64O5bsbmErOx83t0UCvL/mXIq156bbkHuE/YpmKhzu5ft9HjWB3fPe5tLyFqcz/LNJfTo2I6ff+NUrh3Zl5S2CV4PzRzHAt1Endu9bKfHsz54yy3fVMKDizeyYst+enZqx/9OPY1vn5NuQe5TbbwegAm+uycPIiXpywHQkl620+O5/brxQlV5d9M+rvrTcq55/D227DvML6aextKfTOCmMf0szH3MztBN1Lndy3Z6vHjugzeHqobPyPNZuXU/vTq145cXn8Y156STnGQhHgush25MnFNV3ikoISt7I+9v/YwTO7fjtvGZXH12mgW5D1kPPYZ51aO2nnfwqSrLCvaRtTifvG2fcVLnZGZfejpXjbAgj1UW6D7mVY/aet7Bpqoszd/Hg4s3snp7KSd3SebXl57OVWen0S7RgjyW2U1RH2uoR+2H1/VqfKZ5VJW3Nu7likff5Ya/rGRXWQW/vmwwuXeP5/pRGRbmAWBn6D7mVY/aet7BoqrkbtxL1uJ8PigspXeXZP7v8sFcOTzVQjxgLNB9zKsetfW8g0FVyf10Lw9m5/NhYSl9uqbw/y4fwpXDU2mbaP84DyL7VH3Mqx619bxjm6qS88luLnv4Hb7z5PvsO3iU314xhCU/Hs+3R6ZbmAeYnaH7mFc9aut5x6ZQkO8hKzuftUVlpJ6Qwn1XDOGKs+yMPF5YD92YGKeqLN6whznZ+awrLiOtWwp3TMjkirNSSUqwIA+aFvXQRSQZeBtoF97/BVX9ZcQ+04EHgOLwprmq+kRLBm1iw89fWsf8FYVUq5IgwjUj0/jNZUOavZ/fe/d+oqr8Z/1usrLz+XjHAdK7ted3Vw7l8mF9LMjjlJNLLkeBiap6SESSgGUi8oaqvhex33Oqeof7QzR+9fOX1vHMe9s/f1yt+vnj48Pa6X5+7937hary7/W7mRMO8r7d2/PAlUO5zII87jX66WvIofDDpPAvb67TGF+Zv6LQ0Xan+/m9d++1mhpl0Ue7mDJnGd97ehWHj1bx+2+dQfZd5/OtEWkW5sbZTVERSQBWAZnAw6q6oo7dviki44CNwA9V9St/i0VkJjATID09vdmDNv5QXc/9l8jtTvfze+/eKzU1yr/X7+LBxfl8susg/Xp04A9XncElZ/Qm0ULcHMdRoKtqNXCmiHQFXhSRwar60XG7vArMV9WjInIL8DdgYh3HmQfMg9BN0RaP3ngqQaTOsE4QadZ+fu/dt7aaGmXRx7uYkx0K8v49OvDHq8/g4qEW5KZuTfqvQlVLgVzg6xHbS1T1aPjh48BwV0ZnfO2akWmOtjvdz++9+9ZSU6P8a+1OLspaym3PruZYdQ0PXn0m/7nrfC4flmphburlpOXSE6hU1VIRSQEuAO6P2OdkVd0ZfngJsMH1kRrfqb2h2Vh7xel+fu/dR1tNjfL6RzuZk53Pxt2HGNCzA1nTzmTq0N4ktJHGD2DiXqM9dBEZSugSSgKhM/oFqjpbRGYDear6ioj8llCQVwH7gVtV9ZOGjms9dGNCqmuU19eFgjx/zyEye3Xk+xMzLchNnRrqods3Fvmc2/1op31wt4/n1frqbs/XTdU1ymtrd/BQTgEFew4xsFdHZk0ayJQhJzc/yNcugOzZUFYEXVJh0i9g6FXuDtx4yn7ARYxyux/ttA/u9vG8Wl/d7fm6pTbI52Tns2nvYU45sSNzvz2MKYNPpk1LzsjXLoBXZ0Fl+AZvWWHoMVioxwm7u+JjbvejnfbB3T6eV+uruz3flqquUV5aU8yFf3yLO//xAYlt2vDwt89i0Z3jmDq0d8vCHEJn5pURbZ3K8tB2ExfsDN3H3O5HO+2Du308r9ZXd3u+zVVVXcOra3fwUHYBm/cd5msndeKRa8/i66ef1PIQP15ZUdO2m8CxQPcxt/vRTvvgbh/Pq/XV3Z5vU1VV1/DyBzuYu6SALeEgf+y6s/iv01wO8lpdUkOXWerabuKCXXLxMbf70U774G4fz6v11d2er1NV1TW8sKqIC/7wFj96/kOSkxJ47LrhvD5rLF9v6XXyhkz6BSRF/M8vKSW03cQFO0P3Mbf70U774G4fz6v11d2eb2Oqqmt4cU0xc5cUsK3kCKed3Jk/XT+cC089MXohfrzaG5/WcolbVls0poUqa4M8p4Dt+49weu/O3DlpIBeediLSSpd3TPyw2qKJGq/65X5QWV3Di6uLeWhJPoX7yxncpzNP3DCCSaf2ir8gt/67L1igm2bzql/utcrqGv65qoiHcwso3F/OkD5d+NWNpzPxa3EY5GD9dx+xm6Km2bzql3vlWFUN81duZ8Lvc/npwnV0a9+Wv0wfwSt3jGbSqXF8ecX6775hZ+im2bzql7e2Y1Wh1srDSwooLi3njLSu/PrSwYwf1DN+Q/x41n/3DQt002xe9ctby7GqGp5fVcgjSzZRXFrOmWld+c3lgxl/igX5l1j/3TfskotpNq/65dF2tKqap9/bxvgHlvA/L35Er87t+NuMc3jxtvOYMChOr5M3xPrvvmFn6KbZvOqXR8vRqmoWvF/II7mb2FlWwVnpXbnvm0MZO7CHhXhDrP/uG9ZDN3GvorKaBXmhSyu7DlQwvO8J/OCCgYzJtCA3/mM99Fbkdt/a6fG8Wvc7lvvlFZXVPPd+IY/kFrD7wFHOzjiB33/rDEZndvdfkAep5x2kuTjRivO1QHeR231rp8fzat3vWO2XV1RWM3/ldh57axO7DxzlnIxu/PGqMxk1wIdBDsHqeQdpLk608nztpqiL3O5bOz2eV+t+x1q/vKKymr8s28K43y3h3lfX07d7B/5+80ie+965nOfnyytB6nkHaS5OtPJ87QzdRW73rZ0ez6t1v2OlX15RWc2zK0Jn5HsPHmVkv25kTRvGqAHdvR6aM0HqeQdpLk608nwt0F3kdt/a6fG8Wvfb7/3y8mPVPLtiG396ezN7Dx5lVP/uPHTNMM7tHyNBXitIPe8gzcWJVp6vXXJxkdt9a6fH82rdb7/2y8uPVfPE0s2M/d0SfvOvDQzs1ZHnZp7L/Jnnxl6YQ7B63kGaixOtPF87Q3eR231rp8dr7XW/mzq+1nLkWBXPvLeNeW9vZt+hY4zO7M4jk87inH7dPBmPa4LU8w7SXJxo5flaD93EvCPHqnh6eSjISw4fY0xmD+68YCBnZ8R4kBtTB+uhx7B467U3xeGjVTz93jYeDwf52IE9uHPSQEZYkJu6vHYXrHoStBokAYZPh6l/aP7xfNint0D3sXjrtTt1+GgVTy3fxuNLN7P/8DHGndKTOycNZHjfE7wemvGr1+6CvD9/8Virv3jcnFD3aZ/ebor6WLz12htz6GgVDy8pYMz9Ody/6BOG9OnCwtvO46kZ51iYm4aterJp2xvj0z69naH7WLz12utzsKLy8zPy0iOVjB8UOiMflm4hbhzS6qZtb4xP+/QW6D4Wb732SAcrKvnbu1t5YtkWSo9UMmFQT+684BTOTOvaquMwASAJdYe3JHx1mxM+7dPbJRcfi7dee60DFZU8lJ3PmPuX8Pt/b2R4+gm8fPto/vqdcyzMTfMMn9607Y3xaZ/eztB9LN567QcqKnnyna08sXQzByqquODUXsyaNJChqRbipoVqb3y61XLxaZ/eeujGc2Xllfz1nS38ZdmWcJCfyJ2TBjIktYvXQzPGd1rUQxeRZOBtoF14/xdU9ZcR+7QDngKGAyXA1aq6tYXj9hWn/W2/rw/utF/eGvMtK6/kL8u28Jd3tnCwoooLTwsF+eA+Lga5211hp11mt1/X78fzktO5BGnO9XByyeUoMFFVD4lIErBMRN5Q1feO2+cm4DNVzRSRacD9wNVRGK8nnPa3/b4+uNN+ebTnW3akkj+/s4W/hoN88uknMmvSQE7v7fIZudtdYaddZrdf1+/H85LTuQRpzg1o9KaohhwKP0wK/4q8TnMp8Lfw1y8Ak8S3i0s3ndP+tt/XB3faL4/WfEuPHOMP//6UMffnMCc7n9EDevD6rLH86foR7oc5uN8Vdtpldvt1/X48LzmdS5Dm3ABHN0VFJAFYBWQCD6vqiohd+gCFAKpaJSJlQHdgX8RxZgIzAdLT01s28lbktL/t9/XBnfbL3Z5v6ZFjPLF0C0++u5VDR6u4aPBJzJo0kFNP7ux06M3jdlfYaZfZ7df1+/G85HQuQZpzAxzVFlW1WlXPBFKBc0RkcMQudZ2NfyU9VHWeqo5Q1RE9e/Zs+mg9Ul/vO3K70/28Ul+PPHK7W/P97PAxHnjzE8bcv4S5SwoYd0oP3rhzLI9eNzz6YQ71d4Kb2xWur7Mcud3t1/X78bzkdC5BmnMDmtRDV9VSIBf4esRTRUAagIgkAl2A/S6Mzxec9rf9uj54Laf98pbO99bxA/jdok8Yc38Oj+Ru4vxTerLoB2N55NpWCvJabneFnXaZ3X5dvx/PS07nEqQ5N8BJy6UnUKmqpSKSAlxA6Kbn8V4BbgSWA1cCOepVHzIKnPa3/bY+eCSn/fLmzvfEzsmc3qcz/+/1DZRXVvONIScza9JATjmxUyvMrg5ud4Wddpndfl2/H89LTucSpDk3oNEeuogMJXTDM4HQGf0CVZ0tIrOBPFV9JVxtfBoYRujMfJqqbm7ouNZDD46SQ0d5fOkWnlq+lfLKaqYO7c2siZkM9CrIjQmwFvXQVXUtoaCO3P6L476uAL7VkkEGhd976G7ad+goj7+9maeWb6OiqpqLh/bm+0EPcr93nt0eXzTmYZ36qLFv/XeR33vobtl36Cjz3t7M08u3cbSqmovPCAV5Zq8ABzn4v/Ps9viiMQ/r1EeVfeu/i0bfl1PnaoZ9uqbwzk8nejAid+09eJR5b2/imfe2c7SqmkvP7MMdEzMZ0LOj10NrHX8cXM8Ke2nww4+avp/fxxeNebh9TK/eaw/Zj6BrJX7voTfXnoMVzHtrM8+s2MaxqhouCwd5/3gJ8lp+7zy7Pb5ozMM69VFlge4it9cv99qeAxU89tZmnl2xjcrqGi4b1oc7JsRhkNdyuga2V2tluz2+aMzD7WP6dF1yr9h66C7yew/dqT0HKrj31Y8Z+7sl/G35VqYO7U32j8bzh6vOjN8wB/93nt0eXzTmYZ36qLIzdBf5vYfemN0HKng0dxPzV26nqka5PHxGntGjg9dD8we/d57dHl805mGd+qiym6KGXWUVPPbWJv6+cjvVNco3z+rD7RMy6dvdgtwYv7GboqZOu8oqeDS3gPnvF1JTo3zzrFRun5BJevf2Xg8t9jldN90rfh8f+L/L70MW6HFoR2k5j+Zu4rn3C6lR5crhoSBP62ZB7gqn66Z7xe/jA/93+X3KLrnEkR2l5TySW8CC94uoUeVbI1K5bbwFuevu7Vb/T5j/pQ/WrPP7+MD/XX4P2SWXOFdcWs4jSwpYkBf6D/XK4WncNn6ABXm0OF033St+Hx/4v8vvUxboAVb02REeyd3E8+Egv2pEGreOH0DqCRbkUSUJ9Z8B+4Hfxwf+7/L7lPXQA6hw/xHuWbiOCb/P5fm8Qq4+O43cuyfwf5cPsTBvDU7XTfeK38cH/u/y+5SdoQdI4f4jPLykgBdWFdFGhGlnp3Pr+AEx+52qMcvpuule8fv4wP9dfp+ym6IBULj/CHNzCvjn6nCQnxO6tHJyFwtyY4LGbooG1PaSI8xdks/C1cW0aSNcOzKdW2I9yP3eAXZ7fG73wf3+/pmoskCPQdtKDjM3p4CFa4pJaCNcd25fbh0/gBM7J3s9tJbxewfY7fG53Qf3+/tnos4uucSQrfsOM3dJAS+uKSaxjfDtkenccn4AgryW3zvAbo/P7T64398/4wq75BLjtuw7zEM5+bz8wQ4S2wg3jsrglvP70ysoQV7L7x1gt8fndh/c7++fiToLdB/bvPcQc3MKeOmDYtomtmH6eRl87/z+9OoUsCCv5fcOsNvjc7sP7vf3z0Sd9dB9aNPeQ/zwuQ+44A9v8fpHO5kxuh9v/2QC/zv1tOCGOfi/A+z2+Nzug/v9/TNRZ2foPlKw5xBzc/J55cMdtE1sw3fH9ufmsf3p2amd10NrHX7vALs9Prf74H5//0zU2U1RHyjYc5CHcgp45cMdJCcmcMOovtw8rj89OsZJkBtjHLOboj6Vv/sgc3IKeG3tDlKSEpg5rj8zx/anuwV563O7v+30eNYbNy6yQPfAxt0HmZOdz7/W7SQlKYHvjRvAzWP7WZB7xe3+ttPjWW/cuMwuubSiT3cdZE5OPq+v20n7pARuOC+Dm8f2p1uHtl4PLb653d92ejzrjZtmsEsuHvtk1wEeyi7gX+t20qFtAreNH8B3x/TnBAtyf3C7v+30eNYbNy6zQI+iDTsPMCc7nzc+2kXHdoncMSGTm8b0syD3G7f7206PZ71x4zLroUfB+h0HuOXpVVyUtZSl+fv4/sRMlv33BH48eZCFuR+53d92ejzrjRuX2Rm6iz7eUcac7Hze/Hg3ndolMmtiJjPG9KNrewtxX3O7v+30eNYbNy6zm6Iu+Kg4FOT/Xr+bTsmJzBjdjxmj+9GlfZLXQzPGBEyLboqKSBrwFHASUAPMU9WsiH3GAy8DW8KbFqrq7JYMOhZ8VFxGVnY+/wkH+Q8uGMh3RvejS4oF+Vf4vW9tvfGWsffFF5xccqkCfqSqq0WkE7BKRP6jqusj9luqqlPdH6L/rCsqIyt7I4s37KFzciI/vOAUpo/OsCCvj9/71tYbbxl7X3yj0ZuiqrpTVVeHvz4IbAD6RHtgfrS2qJSbnnyfi+cuY+WW/dx14Sks++lE7rxgoIV5Q7Jnf/GXvVZleWi7Hzgdn9/n4RV7X3yjSTdFRSQDGAasqOPpUSLyIbAD+LGqflzHn58JzARIT09v6lg982FhKVnZ+eR8socuKUn86MJTuHF0Bp2TLcQd8Xvf2nrjLWPvi284DnQR6Qj8E/iBqh6IeHo10FdVD4nIFOAlYGDkMVR1HjAPQjdFmz3qVvJBYSlZizey5NO9dG2fxM9aY2UAAAnfSURBVN2TB3HDqL50siBvGr/3ra033jL2vviGox66iCQRCvNnVXVh5POqekBVD4W/fh1IEpEero60Fa3Z/hnT/7qSyx5+hzWFpdw9eRDL/nsit0/ItDBvDr/3ra033jL2vviGk5aLAH8GNqhqnQs1i8hJwG5VVRE5h9D/KEpcHWkrWLXtM7Ky83l7415OaJ/ET74+iBtGZdCxndX1W8TvfWvrjbeMvS++0WgPXUTGAEuBdYRqiwA/A9IBVPUxEbkDuJVQI6YcuEtV323ouH7qoa/atp8HF+ezNH8f3Tq05eax/blhVF86WJAbY3ymRT10VV0GSCP7zAXmNm943snbup+s7C+C/KcXfY3rz7UgB+KvV/zaXe795CBjPBKXyfX+1v1kLc5nWcE+undoy8+mfI3rzu1L+7Zx+XZ8Vbz1il+7C/L+/MVjrf7isYW6iSFxlWArt+wnK3sj7xSU0KNjW/5nyqlce266BXmkhnrFQQz0VU/Wv90C3cSQuEiyFZtLeHBxPss3l9CjYzt+/o1TuXZkX1LaJng9NH+Kt16xVjdtuzE+FehAX76phKzsjby3eb8FeVPEW69YEuoOb7H/TkxsCWSgL99UwoOLN7Jiy356dmrH/049jW+fk25B7tSkX3z5GjoEu1c8fPqXr6Efv92YGBKYQFfVUJBn57Nyy356dWrHLy8+jWvOSSc5yYK8SeKtV1x7ndxaLibGxfx66KrKu5tKyFqcz8qt+zmxcztuPX8A0yzIjTEBFMgfEq2qvFMQukb+/tbPOKlzMvdecjpXn50WE0H+0ppiHnjzU3aUltO7awp3Tx7EZcNicBHLoPTVgzIPr9j75wsxF+iqyrKCfTy4OJ9V20JBPvvS07lqRGwEOYTC/J6F6yivDN2IKy4t556F6wBiK9SD0lcPyjy8Yu+fb8TcD4l+Pq+I6/+8kh2l5fz6ssG89ZPx3DAqI2bCHOCBNz/9PMxrlVdW88Cbn3o0omYKyjrYQZmHV+z9842YO0O/aMhJVNbUcOXwVNolxk6IH29HaXmTtvtWUPrqQZmHV+z9842YO0PvlJzEtSP7xmyYA/TumtKk7b5VXy891vrqQZmHV+z9842YC/QguHvyIFIiLhGlJCVw9+RBHo2omYKyDnZQ5uEVe/98I+YuuQRB7Y3PmG+5BKWvHpR5eMXeP9+I+R66McbEk4Z66HbJxZggWbsA/jgYftU19PvaBf46nokqu+RiTFC43Qe3fnnMsTN0Y4LC7T649ctjjgW6MUHhdh/c+uUxxwLdmKBwuw9u/fKYY4FuTFC43Qe3fnnMsUA3JiiGXgUXz4EuaYCEfr94TvNvYLp9PBN11kM3xpgYYj10Y4yJAxboxhgTEBboxhgTEBboxhgTEBboxhgTEBboxhgTEBboxhgTEBboxhgTEI0GuoikicgSEdkgIh+LyJ117CMiMkdECkRkrYicFZ3hGt+x9bKN8Q0n66FXAT9S1dUi0glYJSL/UdX1x+1zETAw/Gsk8Gj4dxNktl62Mb7S6Bm6qu5U1dXhrw8CG4DIH355KfCUhrwHdBWRk10frfEXWy/bGF9p0jV0EckAhgErIp7qAxQe97iIr4Y+IjJTRPJEJG/v3r1NG6nxH1sv2xhfcRzoItIR+CfwA1U9EPl0HX/kK6t+qeo8VR2hqiN69uzZtJEa/7H1so3xFUeBLiJJhML8WVVdWMcuRUDacY9TgR0tH57xNVsv2xhfcdJyEeDPwAZV/UM9u70C3BBuu5wLlKnqThfHafzI1ss2xlectFxGA9cD60Tkg/C2nwHpAKr6GPA6MAUoAI4A33F/qMaXhl5lAW6MTzQa6Kq6jLqvkR+/jwK3uzUoY4wxTWffKWqMMQFhgW6MMQFhgW6MMQFhgW6MMQFhgW6MMQFhgW6MMQFhgW6MMQEhoQq5By8sshfY1sw/3gPY5+JwvBSUudg8/CUo84DgzMWtefRV1ToXw/Is0FtCRPJUdYTX43BDUOZi8/CXoMwDgjOX1piHXXIxxpiAsEA3xpiAiNVAn+f1AFwUlLnYPPwlKPOA4Mwl6vOIyWvoxhhjvipWz9CNMcZEsEA3xpiA8H2gi0iCiKwRkdfqeK6diDwnIgUisiL8Q6x9qZF5TBeRvSLyQfjXd70YoxMislVE1oXHmVfH8yIic8KfyVoROcuLcTbGwTzGi0jZcZ+JL3+unoh0FZEXROQTEdkgIqMino+Vz6OxecTK5zHouDF+ICIHROQHEftE7TNx8hOLvHYnsAHoXMdzNwGfqWqmiEwD7geubs3BNUFD8wB4TlXvaMXxtMQEVa3vGyQuAgaGf40EHg3/7kcNzQNgqapObbXRNE8WsEhVrxSRtkD7iOdj5fNobB4QA5+Hqn4KnAmhkzigGHgxYreofSa+PkMXkVTgG8AT9exyKfC38NcvAJPCPwPVVxzMI0guBZ7SkPeAriJysteDCiIR6QyMI/Qzf1HVY6paGrGb7z8Ph/OIRZOATaoa+R3xUftMfB3owIPAT4Caep7vAxQCqGoVUAZ0b52hNUlj8wD4ZvifXy+ISForjas5FPi3iKwSkZl1PP/5ZxJWFN7mN43NA2CUiHwoIm+IyOmtOTiH+gN7gb+GL+c9ISIdIvaJhc/DyTzA/59HpGnA/Dq2R+0z8W2gi8hUYI+qrmpotzq2+aqH6XAerwIZqjoUWMwX/+rwo9GqehahfzbeLiLjIp73/WcS1tg8VhNaM+MM4CHgpdYeoAOJwFnAo6o6DDgM/DRin1j4PJzMIxY+j8+FLxtdAjxf19N1bHPlM/FtoAOjgUtEZCvwD2CiiDwTsU8RkAYgIolAF2B/aw7SgUbnoaolqno0/PBxYHjrDtE5Vd0R/n0PoWuD50Ts8vlnEpYK7Gid0TnX2DxU9YCqHgp//TqQJCI9Wn2gDSsCilR1RfjxC4SCMXIfv38ejc4jRj6P410ErFbV3XU8F7XPxLeBrqr3qGqqqmYQ+qdLjqpeF7HbK8CN4a+vDO/jq7MPJ/OIuH52CaGbp74jIh1EpFPt18B/AR9F7PYKcEP4Tv65QJmq7mzloTbIyTxE5KTa+zEicg6hvyslrT3WhqjqLqBQRAaFN00C1kfs5vvPw8k8YuHziHANdV9ugSh+JrHQcvkSEZkN5KnqK4RuojwtIgWEzsyneTq4JoiYxywRuQSoIjSP6V6OrQEnAi+G/14lAn9X1UUicguAqj4GvA5MAQqAI8B3PBprQ5zM40rgVhGpAsqBaX47WQj7PvBs+J/4m4HvxODnAY3PI1Y+D0SkPXAh8L3jtrXKZ2Lf+m+MMQHh20suxhhjmsYC3RhjAsIC3RhjAsIC3RhjAsIC3RhjAsIC3RhjAsIC3RhjAuL/A3qjT525bYCtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画图\n",
    "x_points = np.arange(4,8)\n",
    "\n",
    "y_ = -(weights[1]*x_points + weights[0])/weights[2]\n",
    "plt.plot(x_points,y_)\n",
    "\n",
    "plt.scatter(X[:50,0],X[:50,1],label=\"0\")\n",
    "plt.scatter(X[50:,0],X[50:,1],label = \"1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 最大熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [['no', 'sunny', 'hot', 'high', 'FALSE'],\n",
    "           ['no', 'sunny', 'hot', 'high', 'TRUE'],\n",
    "           ['yes', 'overcast', 'hot', 'high', 'FALSE'],\n",
    "           ['yes', 'rainy', 'mild', 'high', 'FALSE'],\n",
    "           ['yes', 'rainy', 'cool', 'normal', 'FALSE'],\n",
    "           ['no', 'rainy', 'cool', 'normal', 'TRUE'],\n",
    "           ['yes', 'overcast', 'cool', 'normal', 'TRUE'],\n",
    "           ['no', 'sunny', 'mild', 'high', 'FALSE'],\n",
    "           ['yes', 'sunny', 'cool', 'normal', 'FALSE'],\n",
    "           ['yes', 'rainy', 'mild', 'normal', 'FALSE'],\n",
    "           ['yes', 'sunny', 'mild', 'normal', 'TRUE'],\n",
    "           ['yes', 'overcast', 'mild', 'high', 'TRUE'],\n",
    "           ['yes', 'overcast', 'hot', 'normal', 'FALSE'],\n",
    "           ['no', 'rainy', 'mild', 'high', 'TRUE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no', 'sunny', 'hot', 'high', 'FALSE'],\n",
       " ['no', 'sunny', 'hot', 'high', 'TRUE'],\n",
       " ['yes', 'overcast', 'hot', 'high', 'FALSE'],\n",
       " ['yes', 'rainy', 'mild', 'high', 'FALSE'],\n",
       " ['yes', 'rainy', 'cool', 'normal', 'FALSE'],\n",
       " ['no', 'rainy', 'cool', 'normal', 'TRUE'],\n",
       " ['yes', 'overcast', 'cool', 'normal', 'TRUE'],\n",
       " ['no', 'sunny', 'mild', 'high', 'FALSE'],\n",
       " ['yes', 'sunny', 'cool', 'normal', 'FALSE'],\n",
       " ['yes', 'rainy', 'mild', 'normal', 'FALSE'],\n",
       " ['yes', 'sunny', 'mild', 'normal', 'TRUE'],\n",
       " ['yes', 'overcast', 'mild', 'high', 'TRUE'],\n",
       " ['yes', 'overcast', 'hot', 'normal', 'FALSE'],\n",
       " ['no', 'rainy', 'mild', 'high', 'TRUE']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化变量\n",
    "_samples = []\n",
    "_Y = set()   # 标签集合，相当于去重后的y\n",
    "_numXY = {}  # key为(x,y),value为出现次数\n",
    "_N = 0       # 样本数\n",
    "_Ep_ = []    # 样本分布的特征期望值\n",
    "_xyID = {}   # key为(x,y),value为记录ID号\n",
    "_n = 0       # 特征键值(x,y)的个数\n",
    "_C = 0       # 最大特征数\n",
    "_IDxy = {}   # key为(x,y),value 为对应的id号\n",
    "_w = []      # 权重\n",
    "_EPS = 0.005 # 收敛条件\n",
    "_lastw = []  # 上一次w参数值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_samples = dataset \n",
    "\n",
    "for items in _samples:\n",
    "    X = items[1:]\n",
    "    y = items[0]\n",
    "    \n",
    "    _Y.add(y) \n",
    "    \n",
    "    for x in X:\n",
    "        if (x,y) in _numXY :\n",
    "            _numXY[(x,y)] += 1\n",
    "        else:\n",
    "            _numXY[(x,y)] = 1\n",
    "            \n",
    "_N = len(_samples)       # 样本数\n",
    "_n = len(_numXY)        # 特征键值(x,y)的个数\n",
    "_C = max(len(sample) - 1 for sample in _samples)   # 最大特征数\n",
    "_w = [0] * _n               # 权重\n",
    "_lastw = _w[:]        #上一次w参数值\n",
    "\n",
    "_Ep_ = [0]*_n     # 样本分布的特征期望值\n",
    "for i,xy in enumerate(_numXY) : # 计算特征f关于经验分布的期望\n",
    "    _Ep_[i] = _numXY[xy]/_N  \n",
    "    _xyID[xy] = i\n",
    "    _IDxy[i] = xy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最大熵模型 $$P_w = P_w(y|x) = \\frac{1}{Z_w(x)}exp\\big(\\sum_{i=1}^n w_if_i(x,y) \\big),\\quad Z_w(x) = \\sum_y exp\\big(\\sum_{i=1}^n w_if_i(x,y)\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Zx(X): ##计算Z_w(x)\n",
    "    zx = 0 \n",
    "    for y in _Y:\n",
    "        ss = 0\n",
    "        for x in X:            \n",
    "            if (x,y) in _numXY:\n",
    "                ss += _w[_xyID[(x,y)]]\n",
    "        zx += np.exp(ss)\n",
    "    return zx    \n",
    "\n",
    "def _model_pyx(y,X): ##计算p(y|x)\n",
    "    zx = _Zx(X)\n",
    "    ss = 0 \n",
    "    for x in X:\n",
    "        if (x,y) in _numXY:\n",
    "            ss += _w[_xyID[(x,y)]]\n",
    "    pyx = np.exp(ss)/zx \n",
    "    return pyx\n",
    "   \n",
    "# _Ep_ = []    # 样本分布的特征期望值\n",
    "# _xyID = {}   # key为(x,y),value为记录ID号8\n",
    "# _IDxy = {}   # key为(x,y),value 为对应的id号8\n",
    "# _EPS = 0.005 # 收敛条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 给定训练数据集，其\n",
    "### 联合分布 $\\tilde{P}(X =x,Y=y) = \\frac{\\nu(X =x,Y=y)}{N}$\n",
    "### 边缘分布 $\\tilde{P}(X =x) = \\frac{\\nu(X =x)}{N}$ \n",
    "其中，$\\nu(X =x,Y=y)$ 表示训练数据中，样本$(x,y)$出现的频数，$\\nu(X =x)$ 表示输出$X$出现的频数，$N$表示训练样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
