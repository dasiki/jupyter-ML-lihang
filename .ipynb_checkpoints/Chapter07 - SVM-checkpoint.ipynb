{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font><b>第七章 支持向量机(support vector machines,SVM)</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、支持向量机是一种二类分类模型。它的基本模型是**定义在特征空间上的间隔最大的线性分类器**，**间隔最大**使它有别于感知机。\n",
    "\n",
    "2、支持向量机的学习策略就是间隔最大化可形式化为求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小化问题。\n",
    "\n",
    "3、**支持向量机的学习算法是求解凸二次规划的最优化算法**\n",
    "\n",
    "4、**当训练数据线性可分时**，通过硬间隔最大化（hard margin maximization），学习一个线性分类器，即线性可分SVM(linear SVM in linearly separable case)，又称为硬间隔支持向量机; \n",
    "\n",
    "5、**当训练数据近似线性可分时**，通过软间隔最大化（hard margin maximization），也学习一个线性的分类器，即线性支持向量机(lineaer SVM)，又称为软间隔支持向量机。\n",
    "\n",
    "6、**当训练数据线性不可分时**，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。\n",
    "\n",
    "7、当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，**核函数（kernel function)** 表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **一、线性可分SVM与硬间隔最大化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、线性可分SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**1、输入都由输入空间转换到特征空间，SVM的学习是在特征空间进行的。**\n",
    "\n",
    "**2、学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程$w\\bullet x + b =0$ ， 它由法向量$w$和截距$b$决定，可用$(w,b)$ 来表示。**\n",
    "\n",
    "**定义（线性可分SVM）：** 给定线性可分训练集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为 \n",
    "#### $$w^*\\bullet x + b^* = 0 $$\n",
    "以及相应的分类决策函数 \n",
    "#### $$f(x) = sign(w^*\\bullet x + b^*)$$\n",
    "称为**线性可分SVM** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、函数间隔与几何间隔**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**一个点距离分离超平面的远近可以表示为分类预测的确信程度。**\n",
    "\n",
    "在超平面$w\\bullet x + b = 0$确定的情况下，**$|w\\bullet x + b|$能够相对地表示点$x$距离超平面的远近。$w\\bullet x + b$的符号与类$y$的符号是否一致能够表示分类是否正确。** 所以，可用$y(w\\bullet x + b)$来表示分类的正确性及确信度，这就是**函数间隔(functional margin)**.\n",
    "\n",
    "**定义(函数间隔)** 对于给定的训练数据集$T$和超平面$(w,b)$, 定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为\n",
    "#### $$\\hat{\\gamma_i} = y_i(w\\bullet x_i +b) $$\n",
    "定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即\n",
    "#### $$\\hat{\\gamma} = \\mathop{min}\\limits_{i=1,\\cdots,N} \\hat{\\gamma_i}$$\n",
    "\n",
    "<font><i><b>函数间隔可以表示分类预测的正确性及确信度</b></i></font>\n",
    "\n",
    "**定义(几何间隔)** 对于给定的训练数据集$T$和超平面$(w,b)$, 定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为\n",
    "#### $$\\gamma_i = y_i\\Big(\\frac{w}{||w||}\\bullet x_i +\\frac{b}{||w||}\\Big) = \\frac{\\hat{\\gamma_i}}{||w||}$$\n",
    "定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，即\n",
    "#### $$\\gamma = \\mathop{min}\\limits_{i=1,\\cdots,N} \\gamma_i$$\n",
    "其中，$||w||$ 为$w$的$L_2$范数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、间隔最大化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面**\n",
    "\n",
    "约束最优化问题：\n",
    "#### $$\\begin{align} & \\mathop {max}\\limits_{w,b} \\gamma \\\\\n",
    "                  s.t.\\quad  & y_i\\Big(\\frac{w}{||w||}\\bullet x + \\frac{b}{||w||}\\Big)\\geq \\gamma ,i=1,2,\\cdots,N \\end{align}$$ \n",
    "<font><i><b>即希望求得一个离超平面$(w,b)$最远的几何间隔 $\\gamma$ ; 约束条件表示，所有的点离超平面$(w,b)$的距离都大于$\\gamma$ </b></i></font>\n",
    "\n",
    "考虑几何间隔与函数间隔的关系式，上述问题可改写为\n",
    "#### $$\\begin{align} & \\mathop {max}\\limits_{w,b} \\frac{\\hat{\\gamma}}{||w||} \\\\\n",
    "                  s.t.\\quad  & y_i\\big(w\\bullet x + b\\big)\\geq \\hat{\\gamma} ,i=1,2,\\cdots,N \\end{align}$$ \n",
    "\n",
    "<font><i><b> 参看几何间隔定义，$\\gamma = \\frac{\\hat{\\gamma}}{||w||}$</b></i></font>\n",
    "\n",
    "由于最大化$\\frac{1}{||w||}$和最小化$\\frac{1}{2}||w||^2$是等价的，且函数间隔 $\\hat{\\gamma}$ 的取值并不影响最优化问题的解。令$\\hat{\\gamma}=1$  因此，上述问题可改写为：\n",
    "#### $$\\begin{align} & \\mathop {min}\\limits_{w,b}\\frac{1}{2}||w||^2 \\\\\n",
    "                  s.t.\\quad  & y_i\\big(w\\bullet x + b\\big) - 1\\geq 0 ,i=1,2,\\cdots,N \\end{align}$$ \n",
    "这是一个凸二次规划(convex quadratic programming)问题。\n",
    "\n",
    "**算法(线性可分SVM学习算法 - 最大间隔法)**\n",
    "\n",
    "输入：线性可分训练数据集 $T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in R_n , y_i\\in {-1,+1}, i = 1,2,\\cdots,N$\n",
    "\n",
    "输出：最大间隔分离超平面和分类决策函数\n",
    "\n",
    " （1） 构造并求解约束最优化问题：\n",
    "#### $$\\begin{align} & \\mathop {min}\\limits_{w,b}\\frac{1}{2}||w||^2 \\\\\n",
    "          s.t.\\quad  & y_i\\big(w\\bullet x + b\\big) - 1\\geq 0 ,i=1,2,\\cdots,N \\end{align}, i=1,2,\\cdots,N$$ \n",
    "求得最优解 $w^*, b^*$\n",
    "\n",
    " （2） 由此得到分离超平面：\n",
    "#### $$w^*\\bullet x + b^* = 0$$\n",
    "   分类决策函数 \n",
    "#### $$f(x) = sign(w^*\\bullet x + b^*)$$\n",
    "\n",
    "<font><i><b>最大间隔分离超平面的存在唯一性： 线性可分训练数据集的最大间隔分离超平面是存在且唯一的。</b></i></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>支持向量与间隔边界</b></font>\n",
    "在线性可分的情况下，训练数据集的样本点中**与分离超平面距离最近的样本点**的实例称为支持向量(support vector)。\n",
    "\n",
    "对$y_i = +1$ 的正实例点，支持向量在超平面\n",
    "#### $$H_1 : w\\bullet x +b =1$$ \n",
    "上，对$y_i = -1$ 的负实例点，支持向量在超平面\n",
    "#### $$H_2 : w\\bullet x +b = -1$$\n",
    "上。分离超平面与$H_1,H_2$平行且位于他们中央。$H_1,H_2$的距离为$\\frac{2}{||w||}$ 。 $H_1,H_2$ 称为 **间隔边界**\n",
    "\n",
    "在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4、学习的对偶算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入拉格郎日乘子$\\alpha_i \\geq 0 , i= 1,2,\\cdots,N$, 构建拉格郎日函数:\n",
    "#### $$ L(w,b,\\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^N\\alpha_iy_i(w\\bullet x_i +b) + \\sum_{i=1}^N\\alpha_i $$\n",
    "根据拉格郎日对偶性，原始问题的对偶问题是极大极小问题：\n",
    "#### $$ \\mathop{max}\\limits_\\alpha \\mathop{min}\\limits_{w,b} L(w,b,\\alpha)$$\n",
    "\n",
    "<font size=4><b> （1）  求$\\mathop{min}\\limits_{w,b} L(w,b,\\alpha)$ ：</b></font>\n",
    "#### $$    \\begin{align} & \\nabla_wL(w,b,\\alpha) = w -  \\sum_{i=1}^N\\alpha_ix_iy_i = 0 &\\Rightarrow \\quad w = \\sum_{i=1}^N\\alpha_iy_ix_i\\\\\n",
    "                        & \\nabla_bL(w,b,\\alpha) = - \\sum_{i=1}^N\\alpha_iy_i = 0       &\\Rightarrow \\quad \\sum_{i=1}^N\\alpha_iy_i = 0   \\end{align}$$\n",
    "\n",
    "将结果代入公式，得到：\n",
    "#### $$ \\begin{align} L(w,b,\\alpha) &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j - \\sum_{i=1}^N\\alpha_iy_i\\Big(\\big(\\sum_{j=1}^N\\alpha_jx_jy_j\\big)\\bullet x_i +b\\Big) + \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                   &= -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j - \\sum_{i=1}^N\\alpha_iy_ib + \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                   &= -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j + \\sum_{i=1}^N\\alpha_i \\end{align}$$\n",
    "\n",
    "即\n",
    "#### $$\\mathop{min}\\limits_{w,b}L(w,b,\\alpha) = -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j + \\sum_{i=1}^N\\alpha_i $$\n",
    " \n",
    "<font size=4><b> （2）  求$\\mathop{min}\\limits_{w,b} L(w,b,\\alpha)$ 对 $\\alpha$ 的极大，即对偶问题</b></font>\n",
    "\n",
    "#### $$\\begin{align} \\mathop{max}\\limits_\\alpha & -\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j + \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & \\alpha_i \\geq 0, \\quad i=1,2,\\cdots,N \\end{align}$$\n",
    "等价于：\n",
    "#### $$\\begin{align} \\mathop{min}\\limits_\\alpha & \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j + \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & \\alpha_i \\geq 0, \\quad i=1,2,\\cdots,N \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>   算法（线性可分的SVM学习算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：线性可分训练数据集 $T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in R_n , y_i\\in {-1,+1}, i = 1,2,\\cdots,N$\n",
    "\n",
    "输出：最大间隔分离超平面和分类决策函数\n",
    "\n",
    " （1） 构造并求解约束最优化问题： \n",
    "#### $$\\begin{align} \\mathop{min}\\limits_\\alpha & \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j + \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & \\alpha_i \\geq 0, \\quad i=1,2,\\cdots,N \\end{align}$$\n",
    "求得最优解，<font size=4> $ \\alpha^* = (\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T $ </font>\n",
    "\n",
    " （2） 计算\n",
    "#### $$w^* = \\sum_{i=1}^N\\alpha_i^*x_iy_i$$\n",
    "并选择$\\alpha^*$的一个正分量$\\alpha_j^* > 0$, 计算\n",
    "#### $$b^* = y_j - \\sum_{i=1}^N\\alpha_i^*y_i(x_i\\bullet x_j)$$\n",
    "\n",
    " （3） 求得分离超平面\n",
    "#### $$w^*\\bullet x + b^* = 0$$\n",
    "分类决策函数：\n",
    "#### $$f(x) = sign(w^*\\bullet x + b^*) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二、线性可分SVM与软间隔最大化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、线性SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**通常情况是，训练数据中有一些异常点（outlier），将这些点除去后，剩下大部分样本点的组合是线性可分的。**\n",
    "\n",
    "**线性不可分意味着，某些样本点$(x_i,y_i)$不能满足函数间隔大于等于1的约束条件**\n",
    "\n",
    "为解决这个问题，可对每个样本点$(x_i,y_i)$引进一个松弛变量 $\\xi_i \\geq 0$， 这样约束函数变为\n",
    "#### $$y_i(w\\bullet x_i +b)\\geq 1-\\xi_i$$\n",
    "目标函数由原来的<font size=4>$\\frac{1}{2}||w||^2$</font> 变为\n",
    "#### $$\\frac{1}{2}||w||^2 + C\\sum_{i=1}^N\\xi_i$$\n",
    "其中，$c>0$ 称为惩罚参数，$C$ 值大时对误分类的惩罚增大，$C$值小时对误分类的惩罚减少。 **调整后的最小化目标函数包含两层含义：使 <font size=4>$\\frac{1}{2}||w||^2$</font> 尽量小即间隔尽量大，同时使误分类点的个数尽量小，$C$是调和二者的系数。**\n",
    "\n",
    "<font size=4><b>线性不可分的线性SVM的学习问题</b></font> 变成如下凸二次规划问题（原始问题）：\n",
    "#### $$\\begin{align} \\mathop{min}\\limits_{w,b,\\xi} \\quad & \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N\\xi_i \\\\\n",
    "                                              s.t. \\quad & y_i(w\\bullet x_i +b)\\geq 1-\\xi_i \\quad i=1,2,\\cdots,N \\\\\n",
    "                                                         & \\xi_i \\geq 0,\\quad i=1,2,\\cdots,N \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、学习的对偶算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始最优化问题的拉格郎日函数是\n",
    "#### $$L(w,b,\\xi,\\alpha,\\mu) = \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N\\xi_i - \\sum_{i=1}^N \\alpha_i\\big(y_i(w\\bullet x_i +b) - 1+\\xi_i \\big) - \\sum_{i=1}^N \\mu_i\\xi_i$$\n",
    "其中，<font size=4>$\\alpha_i \\geq 0,\\mu_i\\geq 0$</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font size=4><b>求$L(w,b,\\xi,\\alpha,\\mu)$ 对 $w,b,\\xi$ 的极小 ：</b></font>\n",
    "\n",
    "#### $$\\begin{align} & \\nabla_w(w,b,\\xi,\\alpha,\\mu) = w - \\sum_{i=1}^N\\alpha_iy_ix_i =0 & \\Rightarrow w = \\sum_{i=1}^N\\alpha_iy_ix_i \\\\\n",
    "                     & \\nabla_b(w,b,\\xi,\\alpha,\\mu) = - \\sum_{i=1}^N\\alpha_iy_i = 0     & \\Rightarrow \\sum_{i=1}^N\\alpha_iy_i = 0  \\\\\n",
    "                     & \\nabla_\\xi(w,b,\\xi,\\alpha,\\mu) = C - \\sum_{i=1}^N\\alpha_i - \\sum_{i=1}^N \\mu_i = 0      & \\Rightarrow C = \\sum_{i=1}^N\\alpha_i + \\sum_{i=1}^N \\mu_i  \\end{align}$$\n",
    "\n",
    "将<font size=4>$w,b,\\xi$ </font> 代入拉格郎日函数中：\n",
    "#### $$\\begin{align}L(w,b,\\xi,\\alpha,\\mu) =& \\frac{1}{2}||w||^2 + C\\sum_{i=1}^N\\xi_i - \\sum_{i=1}^N \\alpha_i\\big(y_i(w\\bullet x_i +b) - 1+\\xi_i \\big) - \\sum_{i=1}^N \\mu_i\\xi_i  \\\\\n",
    "                                          =& \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jx_ix_j + \\big(\\sum_{i=1}^N\\alpha_i + \\sum_{i=1}^N \\mu_i\\big)\\sum_{i=1}^N\\xi_i - \\sum_{i=1}^N \\alpha_i\\Big(y_i(\\sum_{j=1}^N\\alpha_jy_jx_j \\bullet x_i +b) - 1+\\xi_i \\Big) - \\sum_{i=1}^N \\mu_i\\xi_i\\\\\n",
    "                                          =& - \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i\\alpha_jy_iy_jx_ix_j + \\sum_{i=1}^N \\alpha_i \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**得到原始问题的对偶问题：**\n",
    "#### $$\\begin{align} \\mathop{min}\\limits_\\alpha & \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j - \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & 0\\leq \\alpha_i\\leq C, \\quad i=1,2,\\cdots,N \\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>算法（线性可分的SVM学习算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：线性可分训练数据集 $T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in R_n , y_i\\in {-1,+1}, i = 1,2,\\cdots,N$\n",
    "\n",
    "输出：最大间隔分离超平面和分类决策函数\n",
    "\n",
    " （1） 选择惩罚参数$C > 0$, 构造并求凸二次规划问题 \n",
    "#### $$\\begin{align} \\mathop{min}\\limits_\\alpha & \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j - \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & 0\\leq \\alpha_i\\leq C, \\quad i=1,2,\\cdots,N \\end{align}$$\n",
    " 求得最优解 <font size=4>$\\alpha^* = (\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T$</font>\n",
    "\n",
    " （2） 计算 <font size=4>$ w^* = \\sum_{i=1}^N\\alpha_i^*y_ix_i$</font>\n",
    " \n",
    " 选择 <font size=4>$\\alpha^*$</font> 的一个分量 <font size=4>$\\alpha_j^*$</font> 适合条件 <font size=4>$0<\\alpha_j^*<C$</font>, 计算\n",
    " #### $$b^* = y_j - \\sum_{i=1}^N\\alpha_i^*y_i(x_i\\bullet x_j)$$\n",
    "\n",
    " （3） 求得分离超平面\n",
    "#### $$w^*\\bullet x + b^* = 0$$\n",
    "分类决策函数：\n",
    "#### $$f(x) = sign(w^*\\bullet x + b^*) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、支持向量**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><b>实例点$x_i$到间隔边界的距离 $\\frac{\\xi_i}{||w||}$<b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4、合页损失函数(hinge loss function)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性支持向量机学习还有另一种解释，就是最小化以下目标函数：\n",
    "#### $$\\sum_{i=1}^N[1 - y_i(w\\bullet x_i +b)]_+ + \\lambda||w||^2$$\n",
    "目标函数第一项是经验损失或经验风险，函数\n",
    "#### $$L(y(w\\bullet x +b)) = [1 - yy(w\\bullet x +b)]_+$$ \n",
    "称为**合页损失函数(hinge loss function)** 。下标\"+\"表示以下取正传的函数。\n",
    "#### $$[Z]_+ = \\left\\{\\begin{align} z,\\quad z>0 \\\\\n",
    "                                    0,\\quad z\\leq 0 \\end{align}\\right.$$\n",
    "这就是说，<font color='red'><i><b>当样本点$(x_i,y_i)$ 被正确分类且函数间隔（确信度）$y_i(w\\bullet x_i +b) >1$ 时， 损失为 0，否则损失为 $1 - y_i(w\\bullet x_i +b)$</i></b></font>\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **三、非线性SVM与核函数** p153 - p162 ……10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**非线性分类问题是指通过非线性模型才能很好地进行分类的问题**\n",
    "\n",
    "非线性问题往往不好求解，所以希望能用解线性分类问题的方法解决这个问题。所采取的方法是**进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、核技巧**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例： 设原空间为<font size=4>$\\mathcal{X}\\subset \\mathcal{R^2}, x =(x^{(1)},x^{(2)})^T\\in\\mathcal{X}$</font>, 新空间为<font size=4>$\\mathcal{Z}\\subset\\mathcal{R^2}, z =(z^{(1)},z^{(2)})^T\\in\\mathcal{Z}$</font>， **定义从原空间到新空间的变换（映射）：**\n",
    "#### $$z = \\phi(x) = \\Big(\\big(x^{(1)}\\big)^2,\\big(x^{(2)}\\big)^2\\Big)^T$$\n",
    "经过变换<font size=4>$z = \\phi(x)$</font>， 原空间<font size=4>$\\mathcal{X}\\subset R^2$</font> 变换为新空间 <font size=4>$\\mathcal{Z}\\subset R^2$</font>，原空间的点相应地变成了新空间的点，原空间中的椭圆\n",
    "#### $$w_1\\big(x^{(1)}\\big)^2 + w_2\\big(x^{(2)}\\big)^2 +b = 0$$\n",
    "变换为新空间中的直线：\n",
    "#### $$w_1z^{(1)} + w_2z^{(2)} +b = 0$$\n",
    "\n",
    "<font size=3><i><b> 用线性分类方法求解非线性分类问题分为两步：1）使用一个变换将原空间的数据映射到新空间；2）在新空间用线性分类学习方法从训练数据中学习分类模型。</b></i></font>\n",
    "\n",
    "**定义(核函数)** 设<font size=4>$\\mathcal{X}$</font> 是输入空间（欧氏空间<font size=4>$\\mathcal{R}^n$</font> 的子集或离散集合）, 又设<font size=4>$\\mathcal{H}$</font> 为特征空间（希尔伯特空间），如果存在一个从 <font size=4>$\\mathcal{X}$</font> 到 <font size=4>$\\mathcal{H}$</font> 的映射：\n",
    "#### $$\\phi(x): \\mathcal{X}\\rightarrow \\mathcal{H}$$\n",
    "使得对所有 $x,z\\in\\mathcal{X}$ , 函数 $K(x,z)$ 满足条件\n",
    "#### $$K(x,z) = \\phi(x)\\bullet\\phi(z)$$\n",
    "则称$K(x,z)$ 为核函数，$\\phi(x)$为映射函数，式中$\\phi(x)\\bullet\\phi(z)$为 $\\phi(x)$ 和 $\\phi(z)$ 的内积。$\\phi$为输入空间到特征空间的映射。特征空间 $\\mathcal{H}$通常是高维甚至是无穷维的。\n",
    "\n",
    "**核技巧的想法是，在学习与预测中只定义核函数$K(x,z$，而不显式地定义映射函数$\\phi$** 。直接计算$K(x,z)$容易，而通过$\\phi(x)$ 和 $\\phi(z)$ 计算$K(x,z)$并不容易。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>核技巧在SVM中的应用</b></font>\n",
    "\n",
    "在线性支持向量机对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及到实例与实例之间的内积 <font size=4>$x_i\\bullet x_j$</font> , 可以用核函数 $K(x_i,x_j) = \\phi(x_i)\\bullet \\phi(x_j)$ 来代替，则对偶函数的目标函数为\n",
    "\n",
    "#### $$ \\begin{align}W(\\alpha) &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jx_ix_j - \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                               &= \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) - \\sum_{i=1}^N\\alpha_i \\end{align}$$\n",
    "同样，分类决策函数中的内积也可用核函数代替：\n",
    "#### $$\\begin{align} f(x) &= sign(w^*\\bullet x + b^*) \\\\\n",
    "                          &= sign(\\sum_{i=1}^N\\alpha_iy_ix_i\\bullet x + b^*) \\\\\n",
    "                          &= sign(\\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)\\bullet \\phi(x) + b^*) \\\\\n",
    "                          &= sign(\\sum_{i=1}^N\\alpha_iy_i K(x_i,x) + b^*)\\end{align}$$\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、正定核**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***函数 $K(x,z)$ 满足什么条件才能成为核函数？*** 通常所说的核函数就是正定核函数（positive definite kernel function）.\n",
    "\n",
    "**定义（正定核的等价定义）** 设$\\mathcal{X}\\subset \\mathcal{R}^n, K(x,z)是定义在 \\mathcal{X} \\times \\mathcal{X}$ 上的对称函数，如果对任意$x_i\\in\\mathcal{X}, i=1,2,\\cdots,m, K(x,z)$ 对应的Gram矩阵\n",
    "#### $$K = [K(x_i,x_j)]_{m\\times m}$$\n",
    "是半正定的，则称$K(x,z)$是正定核。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、常用核函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1、多项式核函数(polynomial kernel function)**\n",
    "#### $$K(x,z) = (x\\bullet z +1)^p$$\n",
    "对应的支持向量机是一个$p$次多项式分类器。分为在决策函数成为\n",
    "#### $$f(x) = sign\\big(\\sum_{i=1}^N\\alpha_i^*y_i(x_i\\bullet x +1)^p + b^*)\\big)$$\n",
    "\n",
    "\n",
    "**2、高斯核函数(Gaussian kernel function)**\n",
    "#### $$K(x,z) = exp\\big(-\\frac{||x-z||^2}{2\\sigma ^2}\\big)$$\n",
    "对应的支持向量机是一个$p$次多项式分类器。分为在决策函数成为\n",
    "#### $$f(x) = sign\\big(\\sum_{i=1}^N\\alpha_i^*y_iexp\\big(-\\frac{||x-x_i||^2}{2\\sigma ^2}\\big) + b^*)\\big)$$\n",
    "\n",
    "**3、字符串核函数(string kernel function)** 定义在字符串集合上的核函数。\n",
    "\n",
    "核函数不仅可以定义在欧氏空间上，还可以定义在离散数据集合上。字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。\n",
    "\n",
    "考虑一个有限字符表$\\sum$ . 字符串$s$是从$\\sum$ 中取出的有限个字符的序列，包括空字符串。字符串$s$的长度用$|s|$ 表示，它的元素记作$s(1)s(2)\\cdots s(|s|)$. 两个字符串$s$和$t$的连接记作$st$. 所有长度为$n$的字符串记作$\\sum^n$ , 所有字符串的集合记作 $\\sum^* = \\bigcup_{n=0}^\\infty \\sum^n$\n",
    "\n",
    "考虑字符串$s$ 的子串$u$. 给定一个指标序列 $i = (i_1,i_2,\\cdots,i_{|u|}, 1\\leq i_1 < i_2 <\\cdots <i_|u|\\leq |s|$， $s$的子串定义为 $u = s(i) = s(i_1)s(i_2)\\cdots s(i_{|u|)}$, 其长度记作 $l(i) = i_|u| - i_1 +1$ . 如果$i$是连续的，则 $l(i) = |u|$ ; 否则 $l(i) > |u|$ .\n",
    "\n",
    "假设$\\mathcal{S}$是长度大于或等于$n$的字符串集合，$s$是 $\\mathcal{S}$ 的元素。现在建立字符串集合 $\\mathcal{S}$ 到特征空间 $\\mathcal{H}_n = R^{\\sum^n}$ 的映射$\\phi_n(s)$ . $R^{\\sum^n}$ 表示定义在 $\\sum^n$ 上的实数空间，其每一维对应一个字符串 $u\\in \\sum^n$, 映射 $\\phi_n(s)$ 将字符串$s$对应于空间 $R^{\\sum^n}$ 的一个向量，其$u$维上的取值为\n",
    "#### $$[\\phi_n(s)]_u = \\sum_{i:s(i)=u}\\lambda^{l(i)}$$\n",
    "其中，$0 <\\lambda \\leq 1$ 是一个衰减参数， $l(i)$ 表示字符串 $i$ 的长度，求和在 $s$ 中所有与 $u$ 相同的子串上进行。\n",
    "\n",
    "两个字符串$s$ 和$t$ 上的字符串核函数是基于映射 $\\phi_n$的特征空间的内积： \n",
    "#### $$\\begin{align} k_n(s,t) &= \\sum_{u\\in \\sum^n}[\\phi_n(s)]_u[\\phi_n(t)]_u \\\\\n",
    "                              &= \\sum_{u\\in \\sum^n}\\sum_{(i,j):s(i)=s(j)=u}\\lambda^{l(i)}\\lambda^{l(j)} \\end{align} $$\n",
    "字符串核函数$k_n(s,t)$ 给出了字符串 $s$ 和 $t$ 中长等于 $n$ 的所有子串组成的特征向量的余弦相似度(cosine similarity)。 直观上，两个子串相同的子串越多，它们就越相似，字符串核函数的值就越大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4、非线性支持向量分类机**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>算法（非线性SVM学习算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：线性可分训练数据集 $T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in R_n , y_i\\in {-1,+1}, i = 1,2,\\cdots,N$\n",
    "\n",
    "输出：最大间隔分离超平面和分类决策函数\n",
    "\n",
    " （1） 选取适当的核函数 $K(x,z)$ 和适当的参数 $C$, 构造并求解最优化问题\n",
    "#### $$\\begin{align} \\mathop{min}\\limits_\\alpha & \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jx_ix_jy_iy_j - \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & 0\\leq \\alpha_i\\leq C, \\quad i=1,2,\\cdots,N \\end{align}$$\n",
    " 求得最优解 <font size=4>$\\alpha^* = (\\alpha_1^*,\\alpha_2^*,\\cdots,\\alpha_N^*)^T$</font>\n",
    " \n",
    " （2） 构造决策函数：\n",
    " #### $$f(x) = sign\\Big(\\sum_{i=1}^N\\alpha_i^*y_iK(x,x_i) + b^*\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **四、序列最小优化算法(Sequential Minimal Optimization,SMO)** p162 - p169 ……8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO 算法要解如下凸二次规划的对偶问题：\n",
    "#### $$\\begin{align} \\mathop{min}\\limits_\\alpha & \\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) - \\sum_{i=1}^N\\alpha_i \\\\\n",
    "                                     s.t.       & \\sum_{i=1}^N\\alpha_iy_i = 0 \\\\\n",
    "                                                & 0\\leq \\alpha_i\\leq C, \\quad i=1,2,\\cdots,N \\end{align}$$\n",
    "变量是拉格郎日乘子，一个变量$\\alpha_i$ 对应于一个样本点$(x_i,y_i)$ ；变量的总数等于训练样本容量$N$\n",
    "\n",
    "**整个SMO算法包括两个部分：1）求解两个变量二次规划的解析方法；2）选择变量的启发式方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、两个变量二次规划求解方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不失一般性，假设选择的两个变量是<font size=4>$\\alpha_1,\\alpha_2$</font>, 其他变量 <font size=4> $\\alpha_i(i=3,4,\\cdots,N)$ </font> 是固定的。于是，SMO最优化问题的子问题可以写成：\n",
    "#### $$\\begin{align} \\mathop{min}\\limits_{\\alpha_1,\\alpha_2} \\quad & \\frac{1}{2}\\alpha_1^2K_{11} + \\frac{1}{2}\\alpha_2^2K_{22} + \\alpha_1\\alpha_2y_1y_2K_{12} + \\alpha_1y_1\\sum_{i=3}^N\\alpha_iy_iK_{i1} + \\alpha_2y_2\\sum_{i=3}^N\\alpha_iy_iK_{i2} - (\\alpha_1 + \\alpha_2)\\\\\n",
    "                                     s.t.    \\quad & \\alpha_1y_1 + \\alpha_2y_2 = -\\sum_{i=3}^N\\alpha_iy_i = \\varsigma\\\\\n",
    "                                              \\quad & 0\\leq \\alpha_i\\leq C, \\quad i=1,2 \\end{align}$$\n",
    "\n",
    "其中，$K_{i,j} = K(x_i,x_j), i,j= 1,2,\\cdots,N, \\quad\\varsigma$是常数。目标函数中省略了不含$\\alpha_1,\\alpha_2$ 的常数项。\n",
    "\n",
    "由于只有两个变量$(\\alpha_1,\\alpha_2)$， 由约束函数得到$\\alpha_1,\\alpha_2$ 的关系：\n",
    "##### $$\\alpha_1y_1 + \\alpha_2y_2 = -\\sum_{i=3}^N\\alpha_iy_i = \\varsigma \\Rightarrow \\left\\{\\begin{align} \n",
    "& 若 y_1=y_2 \\quad \\alpha_1 + \\alpha_2 = -\\sum_{i=3}^N\\alpha_iy_i = \\varsigma = k \\\\\n",
    "& 若 y_1\\neq y_2 \\quad \\alpha_1 - \\alpha_2 = -\\sum_{i=3}^N\\alpha_iy_i = \\varsigma = k \\end{align}\\right.$$\n",
    "\n",
    "**如下图：**\n",
    "\n",
    "<img src = \"svm_alpha12.png\"  width=300 height=300> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设问题的初始可靠解为<font size=4>$\\alpha_1^{old}, \\alpha_2^{old}$</font>, 最优解为 <font size=4>$\\alpha_1^{new}, \\alpha_2^{new}$</font>，并且假设在沿着约束方向未经剪辑时$\\alpha_2$ 的最优解为 <font size=4>$\\alpha_2^{new,unc}$</font>\n",
    "\n",
    "由于 <font size=4>$\\alpha_2^{new}$</font> 需要满足不等式$0 \\leq alpha_i\\leq C$ , 所以，最优值 <font size=4>$\\alpha_2^{new}$</font> 的取值范围必须满足条件 \n",
    "#### $$L\\leq \\alpha_2^{new} \\leq H$$\n",
    "其中，$L$与$H$ 是  <font size=4>$\\alpha_2^{new}$</font> 所在的对角线段端点的界。\n",
    "#### $$\\begin{align} & 如果 y_1\\neq y_2 \\quad \\Rightarrow \\quad L = max(0,\\alpha_2^{old} - \\alpha_1^{old}), H = min(C, C+\\alpha_2^{old} - \\alpha_1^{old})\\\\\n",
    "                     & 如果 y_1= y_2 \\quad \\Rightarrow \\quad L = max(0,\\alpha_2^{old} + \\alpha_1^{old} - C), H = min(C,\\alpha_2^{old} + \\alpha_1^{old}) \\end{align}$$\n",
    "                     \n",
    "下面，1）求沿着约束方向未经剪辑即未考虑不等式约束 $0\\leq \\alpha_i\\leq C,i=1,2$ 时，$\\alpha_2$ 的最优解 <font size=4>$\\alpha_2^{new,unc}$</font> , 然后再求剪辑后 $\\alpha_2$ 的解 <font size=4>$\\alpha_2^{new}$</font>\n",
    "\n",
    "记\n",
    "#### $$g(x) = \\sum_{i=1}^N \\alpha_iy_iK(x_i,x) + b$$\n",
    "令\n",
    "#### $$E(i) = g(x_i) - y_i = \\Big(\\sum_{j=1}^N \\alpha_iy_iK(x_j,x_i) + b \\Big) - y_i,\\quad i=1,2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**定理 7.6** 最优化问题沿着约束方向未经剪辑时的解是\n",
    "\n",
    "#### $$ \\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1 - E_2)}{\\eta} $$\n",
    "其中，\n",
    "#### $$\\eta = K_{11} + K_{22} - 2K_{12}= ||\\phi(x_1) - \\phi(x_2)||^2$$ $\\phi(x)$\n",
    "是输入空间到特征空间的映射 $E_i,i=1,2$\n",
    "\n",
    "经剪辑后 $\\alpha_2$ 的解是\n",
    "### $$\\alpha_2^{new} = \\left\\{\\begin{align} & H,\\quad                         & \\alpha_2^{new,unc} >H \\\\\n",
    "                                            & \\alpha_2^{new,unc},             & L\\leq \\alpha_2^{new,unc} \\leq H \\\\\n",
    "                                            & L,                              & \\alpha_2^{new,unc} <L\\end{align}\\right.$$\n",
    "\n",
    "由 <font size=4>$ \\alpha_2^{new}$</font> 求得 <font size=4>$ \\alpha_1^{new} $</font> ：\n",
    "### $$\\alpha_1^{new} = \\alpha_1^{old} + y_1y_2(\\alpha_2^{old} - \\alpha_2^{new})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> <b> 解：</b> </font>\n",
    "\n",
    "####    $$\\begin{align} \n",
    "      &目标函数： W(\\alpha_1,\\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + \\alpha_1\\alpha_2y_1y_2K_{12} + \\alpha_1y_1\\sum_{i=3}^N\\alpha_iy_iK_{i1} + \\alpha_2y_2\\sum_{i=3}^N\\alpha_iy_iK_{i2} - (\\alpha_1 + \\alpha_2) \\\\\n",
    "      & 令v_i = \\sum_{j=3}^N\\alpha_jy_jK_{ji} = \\sum_{j=1}^N\\alpha_jy_jK_{ji} - \\sum_{j=1}^2\\alpha_jy_jK_{ji} = g(x_i) - \\sum_{j=1}^2\\alpha_jy_jK_{ji} - b\\quad i=1,2 \\\\\n",
    "      & W(\\alpha_1,\\alpha_2) = \\frac{1}{2}K_{11}\\alpha_1^2 + \\frac{1}{2}K_{22}\\alpha_2^2 + \\alpha_1\\alpha_2y_1y_2K_{12} + \\alpha_1y_1v_1 + \\alpha_2y_2v_2 - (\\alpha_1 + \\alpha_2) \\\\\n",
    "      &  约束函数: \\alpha_1y_1 + \\alpha_2y_2 = -\\sum_{i=3}^N\\alpha_iy_i = \\varsigma\\  \\Rightarrow \\alpha_1 = \\frac{\\varsigma - \\alpha_2y_2}{y_1} \\quad 代入目标函数:\\\\\n",
    "      & W(\\alpha_2) = \\frac{1}{2}K_{11}\\Big(\\frac{\\varsigma - \\alpha_2y_2}{y_1}\\Big)^2 \n",
    "                      + \\frac{1}{2}K_{22}\\alpha_2^2 \n",
    "                      + \\Big(\\frac{\\varsigma - \\alpha_2y_2}{y_1}\\Big)\\alpha_2y_1y_2K_{12} \n",
    "                      + \\Big(\\frac{\\varsigma - \\alpha_2y_2}{y_1}\\Big)y_1v_1 \n",
    "                      + \\alpha_2y_2v_2 \n",
    "                      - \\Big(\\frac{\\varsigma - \\alpha_2y_2}{y_1} + \\alpha_2\\Big) \\\\\n",
    "      & \\frac{\\partial W}{\\partial \\alpha_2} = K_{11}\\Big(\\frac{\\varsigma - \\alpha_2y_2}{y_1}\\Big)\\Big(\\frac{ - y_2}{y_1}\\Big) \n",
    "                                               + K_{22}\\alpha_2 \n",
    "                                               + y_1y_2K_{12}\\Big(\\frac{\\varsigma - \\alpha_2y_2}{y_1}\\Big) \n",
    "                                               -\\frac{y_2}{y_1}\\alpha_2y_1y_2K_{12} \n",
    "                                               -\\frac{y_2}{y_1}y_1v_1 + y_2v_2 \n",
    "                                               - \\Big(\\frac{ - y_2}{y_1} + 1 \\Big)   \\\\\n",
    "      & = K_{11} \\frac{-y_2\\varsigma + \\alpha_2y_2^2}{y_1^2} \n",
    "          + K_{22}\\alpha_2 \n",
    "          + y_2K_{12}\\varsigma - \\alpha_2y_2^2K_{12} \n",
    "          -\\alpha_2y_2^2K_{12} \n",
    "          - y_2v_1 + y_2v_2 \n",
    "          - (-\\frac{y_2}{y_1} + 1)  \\\\\n",
    "      & =  K_{11}\\alpha_2 - K_{11}y_2\\varsigma \n",
    "          + K_{22}\\alpha_2 \n",
    "          + K_{12}y_2\\varsigma - K_{12}\\alpha_2 \n",
    "          - K_{12}\\alpha_2 \n",
    "          - y_2v_1 + y_2v_2 \n",
    "          - (-\\frac{y_2}{y_1} + 1)  \\\\\n",
    "      & =  K_{11}\\alpha_2 + K_{22}\\alpha_2 - 2K_{12}\\alpha_2  - K_{11}y_2\\varsigma + K_{12}y_2\\varsigma - y_2v_1 + y_2v_2 + \\frac{y_2}{y_1} - 1  \\\\  \n",
    "      & =  K_{11}\\alpha_2 + K_{22}\\alpha_2 - 2K_{12}\\alpha_2  - K_{11}y_2\\varsigma + K_{12}y_2\\varsigma - y_2v_1 + y_2v_2 + y_1y_2 - y_2^2  = 0  \\\\  \n",
    "      & \\Rightarrow \\big(K_{11} + K_{22} - 2K_{12}\\big)\\alpha_2 = y_2\\big(K_{11}\\varsigma - K_{12}\\varsigma + v_1 - v_2 - y_1 + y_2\\big) \\\\\n",
    "      &  = y_2\\Big[y_2 - y_1 + K_{11}\\varsigma - K_{12}\\varsigma + \\big(g(x_1) - \\sum_{j=1}^2\\alpha_jy_jK_{j1} - b\\big) - \\big(g(x_2) - \\sum_{j=1}^2\\alpha_jy_jK_{j2} - b\\big) \\Big]\\\\\n",
    "      &  = y_2\\Big[y_2 - y_1 + K_{11}\\varsigma - K_{12}\\varsigma + \\big(g(x_1) - \\alpha_1y_1K_{11} - \\alpha_2y_2K_{12} - b\\big) - \\big(g(x_2) - \\alpha_1y_1K_{12} - \\alpha_2y_2K_{22} - b\\big) \\Big]\\\\\n",
    "      &  = y_2\\Big[K_{11}\\varsigma - K_{12}\\varsigma - \\alpha_1y_1K_{11} - \\alpha_2y_2K_{12} + \\alpha_1y_1K_{12} + \\alpha_2y_2K_{22} + (g(x_1) - y_1) - (g(x_2) - y_2) \\Big]\\\\\n",
    "      &  = y_2\\Big[K_{11}(\\varsigma - \\alpha_1y_1) - K_{12}(\\varsigma + \\alpha_2y_2 - \\alpha_1y_1) + \\alpha_2y_2K_{22} + (g(x_1) - y_1) - (g(x_2) - y_2) \\Big]\\\\\n",
    "      & 将 \\varsigma = \\alpha_1^{old}y_1 + \\alpha_2^{old}y_2, E_i = g(x_i) - y_i\\quad代入公式 \\\\\n",
    "      & \\big(K_{11} + K_{22} - 2K_{12}\\big)\\alpha_2^{new,unc} = y_2\\Big[(K_{11} + K_{22} - 2K_{12})\\alpha_2^{old}y_2 + E_1 - E_2 \\Big] =  (K_{11} + K_{22} - 2K_{12})\\alpha_2^{old} + y_2(E_1 - E_2) \\\\\n",
    "      & 将 \\eta = K_{11} + K_{22} - 2K_{12} 代入，得到\\\\\n",
    "      &  \\alpha_2^{new,unc} = \\alpha_2^{old} + \\frac{y_2(E_1 - E_2)}{\\eta}\n",
    "      \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、变量的选择方法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。\n",
    "\n",
    "**（1）第一个变量的选择**\n",
    "\n",
    "SMO称选择第1个变量的过程为外层循环。外层循环在**训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1变量**。具体地，检验训练样本点$(x_i,y_i)$ 是否满足KKT条件，即\n",
    "##### $$\\begin{align} \\alpha_i = 0 \\quad\\Leftrightarrow & \\quad y_ig(x_i) \\geq 1 \\\\\n",
    "                     0<\\alpha_i<C \\quad\\Leftrightarrow & \\quad y_ig(x_i) =1 \\\\\n",
    "                     \\alpha_i = C \\quad\\Leftrightarrow & \\quad y_ig(x_i) \\leq1 \\end{align}$$\n",
    "其中，<font size=3> $g(x_i) = \\sum_{j=1}^N\\alpha_jy_jK(x_i,x_j) + b$\n",
    "    \n",
    "**（2）第二个变量的选择**\n",
    "\n",
    "SMO称选择第2个变量的过程为内层循环。 假设在外层循环中已已经找到第1个变量$\\alpha_i$，现在要在内层循环中找第2个变量 $\\alpha_2$ 。 **第2个变量的选择标准是希望能使 $\\alpha_2$ 有足够大的变化** 。 $\\alpha_2^{new}$ 依赖于 $|E_1 - E_2|$，为了加快计算速度，一种简单的做法是选择 $\\alpha_2$ 使其对应的 $|E_1 - E_2|$ 最大。\n",
    "\n",
    "**（3）计算阈值 $b$ 和差值 $E_i$**\n",
    "    \n",
    "在每次完成两个变量的优化后，都要重新计算阈值 $b$ 。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、SMO算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>算法（SMO算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：线性可分训练数据集 $T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中$x_i \\in R_n , y_i\\in {-1,+1}, i = 1,2,\\cdots,N$ 精度$\\epsilon$\n",
    "\n",
    "输出：近似解 $\\hat{\\alpha}$\n",
    "\n",
    "（1） 取初值 $\\alpha^{(0)} = 0 令 k=0$\n",
    "\n",
    "（2） 选取最优化变量 $\\alpha_1^{(k)},\\alpha_2^{(k)}$ ,解析求解两个变量的最优化问题，求得最优解 $\\alpha_1^{(k+1)},\\alpha_2^{(k+1)}$ ，更新$\\alpha$ 为 $\\alpha^{(k+1)}$\n",
    "\n",
    "（3）若在精度$\\epsilon$ 范围内满足停机条件\n",
    "\n",
    "#### $$\\sum_{i=1}^N\\alpha_iy_i = 0,\\quad 0\\leq \\alpha_i \\leq C,\\quad i = 1,2,\\cdots,N$$\n",
    "#### $$y_i\\bullet g(x_i) = \\left\\{\\begin{align} & \\geq 1,\\quad \\{x_i|\\alpha_i = 0\\}\\\\ \n",
    "                                         & =1,\\quad \\{x_i|0<\\alpha_i<C\\} \\\\\n",
    "                                         & \\leq 1,\\quad \\{x_i|\\alpha_i = C\\} \\end{align}\\right.$$\n",
    "其中，\n",
    "#### $$g(x_i) = \\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i) + b $$\n",
    "则转（4）；否则令$k = k+1$ 转（2）\n",
    "\n",
    "（4）取<font size=4> $\\hat{\\alpha} = \\alpha^{k+1}$ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性可分SVM - python ---待做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
