{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font><b>第八章 提升(boosting)方法</b></font> P175-P194 ……20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、提升(boosting)方法是一种常用的统计学习方法。在分类问题中，**它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提升分类的性能**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **一、提升方法AdaBoost算法** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、提升方法的基本思路**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。**\n",
    "\n",
    "\n",
    "1、强可学习（strong learnable）：在概率近似正确（probably approximately correct,PAC) 学习框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概率是**强可学习的**。\n",
    "\n",
    "2、弱可学习（weakly learnable）：一个概率（一个类），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概率是**弱可学习的**。\n",
    "\n",
    "3、对于分类问题而言，给定一个训练样本集，求比较粗糙的弱分类规则（弱分类器）要比求精确的分类规则（强分类器）容易得多。\n",
    "\n",
    "4、提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。\n",
    "\n",
    "5、大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。\n",
    "\n",
    "6、对提升方法来说，有两个问题：\n",
    "    \n",
    " 1） **问题一：在每一轮如何改变训练数据集的权值或概率分布？** AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样，没有正确分类的数据，由于权值加大而受到后一轮弱分类器的更大关注。\n",
    " \n",
    " 2） **问题二：如何将弱分类器组合成一个强分类器？** AdaBoost采用加权多数表决的方法。具体地，加大分类误差率小的弱分类器权值，使其在表决中起较大作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、AdaBoost算法** P180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>   算法（AdaBoost）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集 $T = \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$ ，其中 $x_i \\in \\mathcal{X}\\subseteq R^n, y_i\\in\\mathcal{Y} = \\{-1,+1\\}$ ; 弱学习算法。\n",
    "\n",
    "输出：最终分类器 $G(x)$\n",
    "\n",
    " （1） 初始化训练数据的权值分布\n",
    "#### $$D_1 = (w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),\\quad w_{1i} = \\frac{1}{N}, \\quad i=1,2,\\cdots,N$$\n",
    "<font><i><b>说明：假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$</b></i></font>\n",
    "\n",
    " （2） 对$m = 1,2,\\cdots,M$ (m表示学习的轮数 )\n",
    " \n",
    "$\\quad$ (a) 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器\n",
    "#### $$G_m(x) = \\mathcal{X} \\rightarrow \\{-1,+1\\}$$\n",
    "$\\quad$ (b) 计算 $G_m(x)$ 在训练数据集上的分类误差率\n",
    "#### $$e_m = \\sum_{i=1}^N P(G_m(x_i)\\neq y_i) = \\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$$\n",
    "$w_{mi}$ 表示第$m$轮中第 $i$ 个实例的权值，$\\sum_{i=1}^Nw_{mi} = 1$。 $G_m(x)$ 在加权的训练数据集上的分类误差率是被 $G_m(x)$ 误分类样本的权值之和。\n",
    "\n",
    "$\\quad$ (c) 计算 $G_m(x)$ 的系数\n",
    "#### $$\\alpha_m = \\frac{1}{2}log\\frac{1-e_m}{e_m}$$\n",
    "这里的对数是自然对数。 $\\alpha_m$ 表示 $G_m(x)$ 在最终分类器中的重要性。 当 $e_m \\leq \\frac{1}{2}$ 时，$\\alpha_m \\geq 0$，并且$\\alpha_m$ 随着 $e_m$的减少而增大，所以，分类误差率越小的基本分类器在最终分类器中的作用越大。\n",
    "\n",
    "$\\quad$ (d) 更新训练数据集的权值分布\n",
    "#### $$D_{m+1} = (w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})\\\\\n",
    "w_{m+1,i} = \\frac{w_{mi}}{Z_m}exp(-\\alpha_my_iG_m(x_i)) =\\left\\{\\begin{align} \\frac{w_{mi}}{Z_m}exp(-\\alpha_m) ,\\quad & G_m(x_i) = y_i\\\\\n",
    "                                                                              \\frac{w_{mi}}{Z_m}exp(\\alpha_m)  ,\\quad & G_m(x_i) \\neq y_i \\end{align}\\right. \\quad i=1,2,\\cdots,N$$\n",
    "这里，$Z_m$是规范化因子. \n",
    "#### $$Z_m = \\sum_{i=1}^N w_{mi}exp(-\\alpha_my_iG_m(x_i))$$ \n",
    "它使$D_{m+1}$ 成为一个概率分布\n",
    "\n",
    " （3） 构建基本分类器的线性组合\n",
    "#### $$f(x) = \\sum_{m=1}^M\\alpha_mG_m(x)$$\n",
    "得到最终分类器\n",
    "#### $$G(x) = sign(f(x)) = sign\\Big(\\sum_{m=1}^M\\alpha_mG_m(x)\\Big)$$\n",
    "<font><i><b> 线性分组合$f(x)$ 实现$M$ 个基本分类器的加权表决。系数 $\\alpha_m$ 表示了基本分类器 $g_m(x)$ 的重要性。这里，所有$\\alpha_m$ 之后并不为1。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</b></i></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二、AdaBoost算法的训练误差分析**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **三、AdaBoost算法的解释**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**可以认为，AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二分类学习方法。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、前向分步算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**加法模型（additive model）**\n",
    "#### $$f(x) = \\sum_{m=1}^M\\beta_m b(x;\\gamma_m) \\quad b(x;\\gamma_m)为基函数, \\gamma_m为基函数的参数；\\beta_m为基函数的系数$$\n",
    "\n",
    "在给定训练数据及损失函数$L(y,f(x))$ 的条件下，学习加法模型 $f(x)$ 成为经验风险极小化即损失函数极小化问题：\n",
    "#### $$\\mathop{min}\\limits_{\\beta_m,\\gamma_m} \\sum_{i=1}^N L\\Big(y_i,\\sum_{m=1}^M\\beta_m b(x_i;\\gamma_m)\\Big)$$\n",
    "通常这是一个复杂的优化问题。**前向分步算法（forward stagewise algorithm) 求解这一优化问题的想法是：** 因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，那么就可以简化优化的复杂度。具体地，每步只需要优化如下损失函数：\n",
    "#### $$\\mathop{min}\\limits_{\\beta,\\gamma} \\sum_{i=1}^N L\\Big(y_i,\\beta b(x_i;\\gamma)\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>   算法（前向分步算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集 $T = \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$ ; 损失函数 $L(y,f(x))$ ; 基函数 $\\{b(x;\\gamma)\\}$\n",
    "\n",
    "输出：加法模型 $f(x)$\n",
    "\n",
    " （1） 初始化 $f_0(x) = 0 $\n",
    " \n",
    " （2） 对 $m = 1,2,\\cdots,M$\n",
    " \n",
    " $\\quad$ (a) **极小化损失函数**\n",
    "  #### $$(\\beta_m,\\gamma_m) = arg \\mathop{min}\\limits_{\\beta\\gamma} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta b(x_i; \\gamma))$$\n",
    "  得到参数 <font size=4> $\\beta_m,\\gamma_m$ </font> \n",
    "  \n",
    " $\\quad$ (b) **更新**\n",
    "  #### $$ f_m(x) = f_{m-1}(x) + \\beta_m b(x; \\gamma_m) $$\n",
    "\n",
    " （3） 得到加法模型\n",
    " #### $$f(x) =f_M(x) = \\sum_{m=1}^M\\beta_m b(x; \\gamma_m) $$\n",
    " 这样，前向分步算法将同是求解从$m=1$ 到 $M$ 所有参数 $\\beta_m, \\gamma_m$ 的优化问题简化为逐次求解各个 $\\beta_m,\\gamma_m$ 的优化问题。\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、前向分步算法与AdaBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost算法是前向分步加法算法的特例。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **四、提升树**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1、提升树模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1） 提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。\n",
    "\n",
    "2） **以决策树为基函数的提升方法称为提升树（boosting tree)**。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。基本分类器 $x<v$ 或 $x>v$，可以看作是由一个根结点直接连接两个叶结点的简单决策树，即所谓的决策树桩（decision stump）。 \n",
    "\n",
    "3） 提升树模型可以表示为决策树的加法模型:\n",
    "#### $$f_M(x) = \\sum_{m=1}^M T(x;\\Theta_m)$$ \n",
    "其中，$T(x;\\Theta_m)$ 表示决策树，$\\Theta_m$ 为决策树的参数， $M$ 为树的个数。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2、提升树算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升树采用前向分步算法。首先确定初始提升树 $f_0(x) = 0 $, 第 $m$ 步的模型是\n",
    "#### $$f_m(x) = f_{m-1}(x) + T(x; \\Theta_m)$$\n",
    "其中，$f_{m-1}(x)$ 为当前模型，通过经验风险最小化确定下一棵决策树的参数 $\\Theta_m$ :\n",
    "#### $$\\hat{\\Theta}_m = arg\\mathop{min}\\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_m(x_i)) = arg\\mathop{min}\\limits_{\\Theta_m} \\sum_{i=1}^N L(y_i,f_{m-1}(x_i) + T(x_i; \\Theta_m))$$\n",
    "\n",
    "<font><i><b>不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及一般损失函数的决策问题。</b></i></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>   算法（回归问题的提升树算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集 $T = \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}, x_i\\in\\mathcal{X}\\subseteq R^n, y_i\\in\\mathcal{Y}\\subseteq R$ \n",
    "\n",
    "输出：提升树 $f_M(x)$\n",
    "\n",
    " （1） 初始化 $f_0(x) = 0 $\n",
    " \n",
    " （2） 对 $m = 1,2,\\cdots,M$\n",
    " \n",
    " $\\quad$ (a) 计算残差：\n",
    " #### $$r_{mi} = y_i - f_{m-1}(x_i), i=1,2,\\cdots,N$$\n",
    " $\\quad$ (b) 拟合残差 $r_{mi}$ 学习一个回归树，得到 $T(x; \\Theta_m)$\n",
    " \n",
    " $\\quad$ (c) 更新 $f_m(x) = f_{m-1}(x) + T(x; \\Theta_m) $\n",
    " \n",
    " （3） 得到一个回归问题的提升树 \n",
    " #### $$f_M(x) = \\sum_{m=1}^M T(x; \\Theta_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3、梯度提升**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当损失函数是平方损失和指数损失函数时，每一步的优化很简单。但是，对一般损失函数而言，往往每一步的优化并不那么容易。梯度损失（gradient boosting)算法，这是利用最速下降法的近似方法，其关键是利用损失损失函数的负梯度在当前模型的值\n",
    "#### $$-\\Big[\\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)}\\Big]_{f(x) = f_{m-1}(x)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad$<font size=3><b>   算法（梯度提升算法）</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集 $T = \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}, x_i\\in\\mathcal{X}\\subseteq R^n, y_i\\in\\mathcal{Y}\\subseteq R$ ，损失函数$L(y,f(x))$\n",
    "\n",
    "输出：回归树 $\\hat{f}_(x)$\n",
    "\n",
    " （1） 初始化 \n",
    " #### $$f_0(x) = arg\\mathop{min}\\limits_c \\sum_{i=1}^N L(y_i,c) $$\n",
    " <font><i><b>估计使损失函数极小化的常数值，它是只有一个根结点的树</b></i></font>\n",
    "\n",
    " （2） 对 $m = 1,2,\\cdots,M$\n",
    " \n",
    " $\\quad$(a) 对$i = 1,2,\\cdots,N$ ， 计算\n",
    " #### $$r_{mi} = -\\Big[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}\\Big]_{f(x) = f_{m-1}(x)}$$\n",
    "  \n",
    " <font><i><b>计算损失函数的负剃度在当前模型的值，将它作为残差的估计。对于平方损失函数，它通常就是所说的残差；对于一般损失函数，它就是残差的近似值。</b></i></font>\n",
    " \n",
    " $\\quad$(b) 对$r_{mi}$ 拟合一个回归树，得到第 $m$ 棵树的叶结点区域 $R_{mj}, i=1,2,\\cdots,J$\n",
    "\n",
    "  <font><i><b>估计回归树叶结点区域，以拟合残差的近似值。</b></i></font>\n",
    "  \n",
    " $\\quad$(c) 对 $j = 1,2,\\cdots,J$，计算\n",
    " #### $$c_{mj} = arg \\mathop{min}\\limits_c \\sum_{x_i\\in R_{mj}} L(y_i,f_{m-1}(x_i) +c )$$\n",
    " \n",
    " <font><i><b>利用线性搜索估计叶结点区域的值，使损失函数极小化。</b></i></font>\n",
    "  \n",
    " $\\quad$(d) 更新 <font size=4> $f_m(x) = f_{m-1}(x) + \\sum_{j=1}^J c_{mj}I(x \\in R_{mj})$ </font>\n",
    "\n",
    "\n",
    " （3）得到回归树\n",
    " #### $$\\hat{f}(x) = f_M(x) = \\sum_{m=1}^M\\sum_{j=1}^J c_{mj}I(x\\in R_{mj})$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"exp8.2.png\" width=600 height=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font size=4 color = 'blue'><b>待做 - python </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_input_data_8_2():\n",
    "    data = [(1,5.56),(2,5.70),(3,5.91),(4,6.40),(5,6.80),(6,7.05),(7,8.90),(8,8.70),(9,9.00),(10,9.05)]\n",
    "    \n",
    "    X = [i[0] for i in data]\n",
    "    y = [i[1] for i in data]\n",
    "    \n",
    "    return X,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x239e92b2630>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X,y = get_input_data_8_2()\n",
    "\n",
    "plt.scatter(X,y)\n",
    "\n",
    "# to be continued \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.56, 5.7, 5.91, 6.4, 6.8, 7.05, 8.9, 8.7, 9.0, 9.05]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font size=4 color = 'blue'><b>AdaBoost算法实现 - python </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x224faba7128>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAZq0lEQVR4nO3df4xdZZ3H8fd3h1k6KjBpGRaYKZZV0z9sq4URaLohLnUXf9TSVEQaWa2ysmtgweBirCGoDUk1NeAiRkMhCwhb7VacFJYfi2CjuNJkSmu7WkjQoJ0CyzDYVtaWLeW7f9w77fT2zsx97r3n3ud55vNKmrn33Ken3+cc/HrmnM8519wdERFJ35+1uwAREWkONXQRkUyooYuIZEINXUQkE2roIiKZUEMXEcnEcbUONLMOYBDY7e6LKz5bAawBdpcX3erut0+0vpNPPtlnzZoVVKyIyFS3ZcuWl929p9pnNTd04BpgJ3DiOJ//wN2vqnVls2bNYnBwMOCfFxERM/vdeJ/VdMrFzPqADwETHnWLiEj71HoO/ZvAF4A3JhjzETPbbmYbzGxmtQFmdoWZDZrZ4PDwcGitIiIygUkbupktBl5y9y0TDLsfmOXu84AfA3dVG+Tut7l7v7v39/RUPQUkIiJ1quUc+kJgiZl9EJgGnGhm97j7ZaMD3H1kzPi1wNebW6aISP0OHjzI0NAQBw4caHcpNZs2bRp9fX10dnbW/HcmbejuvhJYCWBm7wX+eWwzLy8/zd1fKL9dQuniqYhIFIaGhjjhhBOYNWsWZtbucibl7oyMjDA0NMSZZ55Z89+rO4duZqvMbEn57dVm9isz+yVwNbCi3vWKiDTbgQMHmDFjRhLNHMDMmDFjRvBvFCGxRdx9E7Cp/PqGMcsPH8WL5GZg627WPPIMz+/Zz+ndXVx34WyWzu9td1kSKJVmPqqeeoMaushUM7B1Nyvv28H+g4cA2L1nPyvv2wGgpi7R0a3/IhNY88gzh5v5qP0HD7HmkWfaVJGk7umnn2bBggUcf/zxfOMb32jqunWELjKB5/fsD1ouMpnp06dzyy23MDAw0PR16whdZAKnd3cFLZc8DGzdzcKvPc6ZX/wPFn7tcQa27p78L9XolFNO4T3veU9QHLFWaugiE7juwtl0dXYctayrs4PrLpzdpoqkaKPXTXbv2Y9z5LpJM5t6UdTQRSawdH4vq5fNpbe7CwN6u7tYvWyuLohmLOXrJjqHLjKJpfN71cCnkCKum3z7299m7dq1ADz44IOcfvrpda9rIjpCFxEZo4jrJldeeSXbtm1j27ZthTVzUEMXETlK0ddNXnzxRfr6+rjpppu48cYb6evrY9++fU1Zt065iIiMMXp6rai7g0899VSGhoaasq5KaugiIhVSvW6iUy4iIplQQxcRyYQauohIJtTQRUQyoYYuIpIJNXTJRpEPVBJp1Kc//WlOOeUU5syZU9i/oYYuWUj5gUoyNaxYsYKHH3640H9DDV2ykPIDlSRC29fDzXPgK92ln9vXN7zK888/n+nTpzehuPHpxiLJgr6IQppm+3q4/2o4WP5vZ++u0nuAeZe0r64a6AhdsqAvopCmeWzVkWY+6uD+0vLIqaFLFvRFFNI0e8d5zsp4yyOiUy6ShaIfqCRTyEl9pdMs1ZZHTg1dspHqA5UkMotuOPocOkBnV2l5A5YvX86mTZt4+eWX6evr46tf/SqXX355g8UeTQ1dGjawdbeOjCUfoxc+H1tVOs1yUl+pmTd4QXTdunVNKG5iaujSkNH892hkcDT/DaipS7rmXRJ9oqUaXRSVhij/LRIPNXRpiPLfkgp3b3cJQeqpVw1dGqL8t6Rg2rRpjIyMJNPU3Z2RkRGmTZsW9Pd0Dl0act2Fs486hw7Kf0t8+vr6GBoaYnh4uN2l1GzatGn09YVFJdXQpSHKf0sKOjs7OfPMM9tdRuHU0KVhyn+LxKHmhm5mHcAgsNvdF1d8djxwN3A2MAJ8zN2fa2KdIklQJl/aKeSi6DXAznE+uxz4g7u/HbgZ+HqjhYmkRs9kl3arqaGbWR/wIeD2cYZcBNxVfr0BWGRm1nh5IulQJl/ardYj9G8CXwDeGOfzXmAXgLu/DuwFZlQOMrMrzGzQzAZTutosUgtl8qXdJm3oZrYYeMndt0w0rMqyYwKf7n6bu/e7e39PT09AmSLxUyZf2q2WI/SFwBIzew74PnCBmd1TMWYImAlgZscBJwGvNLFOkejpmezSbpM2dHdf6e597j4LuBR43N0vqxi2Efhk+fXF5TFp3JIl0iRL5/eyetlceru7MKC3u4vVy+Yq5SItU3cO3cxWAYPuvhG4A/iemT1L6cj80ibVJ5IUZfKlnYIaurtvAjaVX98wZvkB4KPNLEzk+oEdrNu8i0PudJix/NyZ3Lh0brvLEomW7hSVKF0/sIN7nvz94feH3A+/V1MXqU5PW5Qordtc5TsdJ1guImroEqlD41xTH2+5iKihS6Q6xrnReLzlIqKGLpFafu7MoOUioouiEqnRC59KuYjUztp1/09/f78PDg625d8WEUmVmW1x9/5qn+kIXar6+Npf8PPfHHl6w8K3TefezyxoY0Xto2ecSyp0Dl2OUdnMAX7+m1f4+NpftKmi9tEzziUlauhyjMpmPtnynOkZ55ISNXSRCegZ55ISNXSRCegZ55ISNXQ5xsK3TQ9anjM941xSooYux7j3MwuOad5TNeWiZ5xLSpRDFxFJiHLoEqyo7HXIepX/Fgmjhi7HGM1ej8b1RrPXQEMNNWS9RdUgkjOdQ5djFJW9Dlmv8t8i4dTQ5RhFZa9D1qv8t0g4NXQ5RlHZ65D1Kv8tEk4NXY5RVPY6ZL3Kf4uE00VROcboRcdmJ0xC1ltUDSI5Uw5dRCQhyqEXIIaMdGgNMdQsIsVRQ69DDBnp0BpiqFlEiqWLonWIISMdWkMMNYtIsdTQ6xBDRjq0hhhqFpFiqaHXIYaMdGgNMdQsIsVSQ69DDBnp0BpiqFlEiqWLonWIISMdWkMMNYtIsZRDFxFJSEM5dDObBvwUOL48foO7f7lizApgDbC7vOhWd7+9kaKl+a4f2MG6zbs45E6HGcvPncmNS+c2PDaWfHssdYi0Sy2nXF4DLnD3V82sE3jCzB5y9ycrxv3A3a9qfonSDNcP7OCeJ39/+P0h98PvKxt1yNhY8u2x1CHSTpNeFPWSV8tvO8t/2nOeRuq2bvOumpeHjI0l3x5LHSLtVFPKxcw6zGwb8BLwqLtvrjLsI2a23cw2mNnMcdZzhZkNmtng8PBwA2VLqEPjXCuptjxkbCz59ljqEGmnmhq6ux9y93cDfcA5ZjanYsj9wCx3nwf8GLhrnPXc5u797t7f09PTSN0SqMOs5uUhY2PJt8dSh0g7BeXQ3X0PsAl4f8XyEXd/rfx2LXB2U6qTpll+btVfmqouDxkbS749ljpE2mnShm5mPWbWXX7dBbwPeLpizGlj3i4BdjazSGncjUvnctl5Zxw+yu4w47LzzqiaXAkZu3R+L6uXzaW3uwsDeru7WL1sbssvRMZSh0g7TZpDN7N5lE6hdFD6P4D17r7KzFYBg+6+0cxWU2rkrwOvAJ9196fHXSnKoYuI1GOiHLpuLKpTUZnnkPx3kesOmV+K2yI529fDY6tg7xCc1AeLboB5l7S7KmkDfcFFkxWVeQ7Jfxe57pD5pbgtkrN9Pdx/NRwsJ3b27iq9BzV1OYoezlWHojLPIfnvItcdMr8Ut0VyHlt1pJmPOri/tFxkDDX0OhSVeQ7Jfxe57pD5pbgtkrN3KGy5TFlq6HUoKvMckv8uct0h80txWyTnpL6w5TJlqaHXoajMc0j+u8h1h8wvxW2RnEU3QGfF/0F2dpWWi4yhi6J1KOrZ4qMX+4pIdoSsO2R+KW6L5Ixe+FTKRSah2KKISEIUWxQgjmy5JE55+KipoU8RMWTLJXHKw0dPF0WniBiy5ZI45eGjp4Y+RcSQLZfEKQ8fPTX0KSKGbLkkTnn46KmhTxExZMslccrDR08XRaeIGLLlkjjl4aOnHLqISEKmdA69qDx1yHpjea63suWRyT3Tnfv8QrRoW2Td0IvKU4esN5bneitbHpncM925zy9EC7dF1hdFi8pTh6w3lud6K1semdwz3bnPL0QLt0XWDb2oPHXIemN5rrey5ZHJPdOd+/xCtHBbZN3Qi8pTh6w3lud6K1semdwz3bnPL0QLt0XWDb2oPHXIemN5rrey5ZHJPdOd+/xCtHBbZH1RtKg8dch6Y3mut7Llkck90537/EK0cFsohy4ikpApnUMvivLtIol44FrYcif4IbAOOHsFLL6p8fVGmLNXQ6+D8u0iiXjgWhi848h7P3TkfSNNPdKcfdYXRYuifLtIIrbcGba8VpHm7NXQ66B8u0gi/FDY8lpFmrNXQ6+D8u0iibCOsOW1ijRnr4ZeB+XbRRJx9oqw5bWKNGevi6J1UL5dJBGjFz6bnXKJNGevHLqISEIayqGb2TTgp8Dx5fEb3P3LFWOOB+4GzgZGgI+5+3MN1l1VaP47tWeAh2TLc98WheZ8Q7LJRdVR5PwizEg3Tejcct4WFWo55fIacIG7v2pmncATZvaQuz85ZszlwB/c/e1mdinwdeBjzS42NP+d2jPAQ7LluW+LQnO+Idnkouoocn6RZqSbInRuOW+LKia9KOolr5bfdpb/VJ6nuQi4q/x6A7DIrPlxi9D8d2rPAA/Jlue+LQrN+YZkk4uqo8j5RZqRborQueW8LaqoKeViZh1mtg14CXjU3TdXDOkFdgG4++vAXmBGlfVcYWaDZjY4PDwcXGxo/ju1Z4CHZMtz3xaF5nxDsslF1VHk/CLNSDdF6Nxy3hZV1NTQ3f2Qu78b6APOMbM5FUOqHY0f04Xc/TZ373f3/p6enuBiQ/PfqT0DPCRbnvu2KDTnG5JNLqqOIucXaUa6KULnlvO2qCIoh+7ue4BNwPsrPhoCZgKY2XHAScArTajvKKH579SeAR6SLc99WxSa8w3JJhdVR5HzizQj3RShc8t5W1RRS8qlBzjo7nvMrAt4H6WLnmNtBD4J/AK4GHjcC8hDhua/U3sGeEi2PPdtUWjONySbXFQdRc4v0ox0U4TOLedtUcWkOXQzm0fpgmcHpSP69e6+ysxWAYPuvrEcbfweMJ/Skfml7v7bidarHLqISLiGcujuvp1So65cfsOY1weAjzZSpIiINCb7W/+Tu5lGWiPkZpMYbkwp8maa1G6cimF/RCrrhp7czTTSGiE3m8RwY0qRN9OkduNUDPsjYlk/bTG5m2mkNUJuNonhxpQib6ZJ7capGPZHxLJu6MndTCOtEXKzSQw3phR5M01qN07FsD8ilnVDT+5mGmmNkJtNYrgxpcibaVK7cSqG/RGxrBt6cjfTSGuE3GwSw40pRd5Mk9qNUzHsj4hl3dCXzu9l9bK59HZ3YUBvdxerl83VBdGpbt4l8OFb4KSZgJV+fviW6hfVQsbGUG/o+KLml9p6M6EvuBARSUhDNxaJTHkhX4YRi9RqjiVbHksddVJDF5lIyJdhxCK1mmPJlsdSRwOyPocu0rCQL8OIRWo1x5Itj6WOBqihi0wk5MswYpFazbFky2OpowFq6CITCfkyjFikVnMs2fJY6miAGrrIREK+DCMWqdUcS7Y8ljoaoIYuMpHFN0H/5UeObq2j9D7Gi4ujUqs5lmx5LHU0QDl0EZGEKIcuxUoxu1tUzUXlv1PcxtJyaujSmBSzu0XVXFT+O8VtLG2hc+jSmBSzu0XVXFT+O8VtLG2hhi6NSTG7W1TNReW/U9zG0hZq6NKYFLO7RdVcVP47xW0sbaGGLo1JMbtbVM1F5b9T3MbSFmro0pgUs7tF1VxU/jvFbSxtoRy6iEhCJsqh6whd8rF9Pdw8B77SXfq5fX3r11tUDSI1UA5d8lBUVjtkvcqLS5vpCF3yUFRWO2S9yotLm6mhSx6KymqHrFd5cWkzNXTJQ1FZ7ZD1Ki8ubaaGLnkoKqsdsl7lxaXN1NAlD0VltUPWq7y4tJly6CIiCWkoh25mM83sJ2a208x+ZWbXVBnzXjPba2bbyn/0O2bqUsxTKy9ePG23qNWSQ38d+Ly7P2VmJwBbzOxRd/91xbifufvi5pcoLZdinlp58eJpu0Vv0iN0d3/B3Z8qv/4jsBPoLbowaaMU89TKixdP2y16QRdFzWwWMB/YXOXjBWb2SzN7yMzeOc7fv8LMBs1scHh4OLhYaZEU89TKixdP2y16NTd0M3sL8EPgc+6+r+Ljp4C3uvu7gG8BA9XW4e63uXu/u/f39PTUW7MULcU8tfLixdN2i15NDd3MOik183vd/b7Kz919n7u/Wn79INBpZic3tVJpnRTz1MqLF0/bLXq1pFwMuAPY6e5VH+xsZqeWx2Fm55TXO9LMQqWFUsxTKy9ePG236E2aQzezvwJ+BuwA3igv/hJwBoC7f9fMrgI+SykRsx+41t3/a6L1KocuIhJuohz6pLFFd38CsEnG3ArcWl95Urft60sJg71DpfOYi26Y2kdLD1wLW+4sfSmzdZS++q3RbwsSSYieh54qZYKP9sC1MHjHkfd+6Mh7NXWZIvQsl1QpE3y0LXeGLRfJkBp6qpQJPpofClsukiE19FQpE3w06whbLpIhNfRUKRN8tLNXhC0XyZAaeqqUCT7a4pug//IjR+TWUXqvC6Iyheh56CIiCWkohz6VDGzdzZpHnuH5Pfs5vbuL6y6czdL5GT1YMvfceu7zi4G2cdTU0MsGtu5m5X072H+wlIrYvWc/K+/bAZBHU889t577/GKgbRw9nUMvW/PIM4eb+aj9Bw+x5pFn2lRRk+WeW899fjHQNo6eGnrZ83v2By1PTu659dznFwNt4+ipoZed3t0VtDw5uefWc59fDLSNo6eGXnbdhbPp6jz6JpSuzg6uu3B2mypqstxz67nPLwbaxtHTRdGy0Quf2aZcRi9a5ZpQyH1+MdA2jp5y6CIiCZkoh65TLiIp2L4ebp4DX+ku/dy+Po11S0vplItI7IrMfytbnhUdoYvErsj8t7LlWVFDF4ldkflvZcuzooYuErsi89/KlmdFDV0kdkXmv5Utz4oaukjsinz2vZ6rnxXl0EVEEqIcuojIFKCGLiKSCTV0EZFMqKGLiGRCDV1EJBNq6CIimVBDFxHJhBq6iEgmJm3oZjbTzH5iZjvN7Fdmdk2VMWZmt5jZs2a23czOKqZcaYieey2StVqeh/468Hl3f8rMTgC2mNmj7v7rMWM+ALyj/Odc4DvlnxILPfdaJHuTHqG7+wvu/lT59R+BnUDlF21eBNztJU8C3WZ2WtOrlfrpudci2Qs6h25ms4D5wOaKj3qBXWPeD3Fs08fMrjCzQTMbHB4eDqtUGqPnXotkr+aGbmZvAX4IfM7d91V+XOWvHPPUL3e/zd373b2/p6cnrFJpjJ57LZK9mhq6mXVSaub3uvt9VYYMATPHvO8Dnm+8PGkaPfdaJHu1pFwMuAPY6e43jTNsI/CJctrlPGCvu7/QxDqlUXrutUj2akm5LAT+DthhZtvKy74EnAHg7t8FHgQ+CDwL/An4VPNLlYbNu0QNXCRjkzZ0d3+C6ufIx45x4MpmFSUiIuF0p6iISCbU0EVEMqGGLiKSCTV0EZFMqKGLiGRCDV1EJBNq6CIimbBShLwN/7DZMPC7tvzjkzsZeLndRRRI80tXznMDza8Wb3X3qg/DaltDj5mZDbp7f7vrKIrml66c5waaX6N0ykVEJBNq6CIimVBDr+62dhdQMM0vXTnPDTS/hugcuohIJnSELiKSCTV0EZFMTOmGbmYdZrbVzB6o8tkKMxs2s23lP3/fjhobYWbPmdmOcv2DVT43M7vFzJ41s+1mdlY76qxHDXN7r5ntHbP/kvquPTPrNrMNZva0me00swUVnye776Cm+SW7/8xs9pi6t5nZPjP7XMWYQvZfLd9YlLNrgJ3AieN8/gN3v6qF9RThr919vBsZPgC8o/znXOA75Z+pmGhuAD9z98Utq6a5/gV42N0vNrM/B95U8Xnq+26y+UGi+8/dnwHeDaWDRmA38KOKYYXsvyl7hG5mfcCHgNvbXUsbXQTc7SVPAt1mdlq7i5rqzOxE4HxK3+WLu/+fu++pGJbsvqtxfrlYBPzG3Svvii9k/03Zhg58E/gC8MYEYz5S/nVog5nNbFFdzeTAf5rZFjO7osrnvcCuMe+HystSMNncABaY2S/N7CEze2cri2vQXwLDwL+WTwnebmZvrhiT8r6rZX6Q7v4b61JgXZXlhey/KdnQzWwx8JK7b5lg2P3ALHefB/wYuKslxTXXQnc/i9Kvd1ea2fkVn1f7rthUcqyTze0pSs+8eBfwLWCg1QU24DjgLOA77j4f+F/gixVjUt53tcwv5f0HQPlU0hLg36t9XGVZw/tvSjZ0YCGwxMyeA74PXGBm94wd4O4j7v5a+e1a4OzWltg4d3++/PMlSufwzqkYMgSM/c2jD3i+NdU1ZrK5ufs+d3+1/PpBoNPMTm55ofUZAobcfXP5/QZKDbByTJL7jhrml/j+G/UB4Cl3/58qnxWy/6ZkQ3f3le7e5+6zKP1K9Li7XzZ2TMX5rCWULp4mw8zebGYnjL4G/hb474phG4FPlK+4nwfsdfcXWlxqsFrmZmanmpmVX59D6b/1kVbXWg93fxHYZWazy4sWAb+uGJbkvoPa5pfy/htjOdVPt0BB+2+qp1yOYmargEF33whcbWZLgNeBV4AV7aytDn8B/Kj8v4njgH9z94fN7B8B3P27wIPAB4FngT8Bn2pTraFqmdvFwGfN7HVgP3Cpp3Vb9D8B95Z/bf8t8KlM9t2oyeaX9P4zszcBfwP8w5hlhe8/3fovIpKJKXnKRUQkR2roIiKZUEMXEcmEGrqISCbU0EVEMqGGLiKSCTV0EZFM/D/SeH5K045XxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "def get_data():\n",
    "    # 加载数据\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "    df[\"label\"] = iris.target\n",
    "\n",
    "    # 重新定义列名\n",
    "    df.columns = [\"sepal length\",\"sepal width\",\"petal length\",\"petal width\",\"label\"]\n",
    "\n",
    "    # 定义输入数据集 取第0，1，及最后一列，前100条数据\n",
    "    data = np.array(df.iloc[:100,[0,1,-1]])\n",
    "\n",
    "    X=data[:,:-1]\n",
    "    y=data[:,-1]\n",
    "\n",
    "    return X,y\n",
    "\n",
    "\n",
    "X,y = get_data()\n",
    "y = np.array([1 if i==1 else -1 for i in y]) ##  y = {-1,+1}\n",
    "\n",
    "# 划分训练集、测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "# 画图\n",
    "plt.scatter(X[:50,0],X[:50,1],label=\"-1\")\n",
    "plt.scatter(X[50:,0],X[50:,1],label=\"1\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1: 初始化训练数据的权值分布\n",
    "#### $$D_1 = (w_{11},\\cdots,w_{1i},\\cdots,w_{1N}),\\quad w_{1i} = \\frac{1}{N}, \\quad i=1,2,\\cdots,N$$\n",
    "<font><i><b>说明：假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器$G_1(x)$</b></i></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125,\n",
       "        0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_w = []                 # 存储每一轮学得中，学得到的弱分类器的权重\n",
    "_M,_N = X_train.shape   # 训练数据集的形状 M行，N列\n",
    "\n",
    "_iter_num = 50          # 定义迭代次数，即step2中学习的轮数\n",
    "learning_rate =1.0      # 学习率\n",
    "_alpha = []              # 弱分类器G(x)的系数\n",
    "\n",
    "clf_sets = []             # 弱分类器集合\n",
    "\n",
    "# 初始化训练数据集的权重\n",
    "w0 = np.ones(_M)/_M\n",
    "_w.append(w0)\n",
    "_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step2 :对$m = 1,2,\\cdots,M$ (m表示学习的轮数 )    \n",
    " \n",
    "$\\quad$ (a) 使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器   <font size=3>$G_m(x) = \\mathcal{X} \\rightarrow \\{-1,+1\\}$</font>\n",
    "\n",
    "$\\quad$ (b) 计算 $G_m(x)$ 在训练数据集上的分类误差率  <font size=3> $e_m = \\sum_{i=1}^N P(G_m(x_i)\\neq y_i) = \\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$ </font>\n",
    "\n",
    "$\\quad$ $\\quad$ $w_{mi}$ 表示第$m$轮中第 $i$ 个实例的权值，$\\sum_{i=1}^Nw_{mi} = 1$。 $G_m(x)$ 在加权的训练数据集上的分类误差率是被 $G_m(x)$ 误分类样本的权值之和。\n",
    "\n",
    "$\\quad$ (c) 计算 $G_m(x)$ 的系数    <font size=3> $\\alpha_m = \\frac{1}{2}log\\frac{1-e_m}{e_m}$ </font>\n",
    "\n",
    "$\\quad$ $\\quad$ 这里的对数是自然对数。 $\\alpha_m$ 表示 $G_m(x)$ 在最终分类器中的重要性。 当 $e_m \\leq \\frac{1}{2}$ 时，$\\alpha_m \\geq 0$，并且$\\alpha_m$ 随着 $e_m$的减少而增大，所以，分类误差率越小的基本分类器在最终分类器中的作用越大。\n",
    "\n",
    "$\\quad$ (d) 更新训练数据集的权值分布 <font size=3> $D_{m+1} = (w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})\\\\\n",
    "w_{m+1,i} = \\frac{w_{mi}}{Z_m}exp(-\\alpha_my_iG_m(x_i)) =\\left\\{\\begin{align} \\frac{w_{mi}}{Z_m}exp(-\\alpha_m) ,\\quad & G_m(x_i) = y_i\\\\\n",
    "                                                                              \\frac{w_{mi}}{Z_m}exp(\\alpha_m)  ,\\quad & G_m(x_i) \\neq y_i \\end{align}\\right. \\quad i=1,2,\\cdots,N$</font>\n",
    "\n",
    "这里，$Z_m$是规范化因子.  <font size=3> $Z_m = \\sum_{i=1}^N w_{mi}exp(-\\alpha_my_iG_m(x_i))$ </font>\n",
    " \n",
    "它使$D_{m+1}$ 成为一个概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = X_train[:,0]\n",
    "# labels = y_train\n",
    "\n",
    "# 定义 基本分类器 并返回该分类器误差 step2-a,step2-b \n",
    "def _G(features,labels,weight):\n",
    "    \"\"\"对某特征进行分类\n",
    "    输入：features 某一特征（某一列）; \n",
    "          labels:{-1,+1}, 即y\n",
    "          weight: 该轮的权重\n",
    "          \n",
    "    输出：best_v 分类阈值 x<best_v -> -1 else 1\n",
    "          error : 误差\n",
    "          clf_result: 分类器结果\n",
    "    \"\"\"    \n",
    "    min_error,best_v = 10000, 0.0 # 定义初始值\n",
    "    feature_len = len(features)   # 训练特征数\n",
    "    clf_result = None             # 分类结果\n",
    "\n",
    "    # 单维feature \n",
    "    min_feature = min(features)\n",
    "    max_feature = max(features)\n",
    "\n",
    "    n_step = np.round((max_feature - min_feature + learning_rate)/learning_rate,0)\n",
    "    # print(min_feature,max_feature,learning_rate,n_step)\n",
    "\n",
    "    for i in range(np.int(n_step)):\n",
    "        v = min_feature + i*learning_rate\n",
    "        result = np.array([1 if i>v else -1 for i in features])  # 备注1\n",
    "\n",
    "        err = np.sum(np.array([weight[i] if result[i]!=labels[i] else 0 for i in range(len(result))])) # 备注2\n",
    "\n",
    "        if err < min_error:\n",
    "            min_error = err \n",
    "            best_v = v\n",
    "            clf_result = result\n",
    "\n",
    "        # print(\"err:\",err,\"min_err\",min_error,\"v:\",v, \"best_v:\",best_v)\n",
    "        \n",
    "    return min_error,best_v,clf_result\n",
    "\n",
    "# min_error,best_v,clf_result = _G(features,labels,_w[0])      #########test\n",
    "\n",
    "# 计算弱分类器的系数 见step2-c\n",
    "def cal_alpha(error):\n",
    "    return 0.5*np.log((1-error)/error)\n",
    "\n",
    "# _alpha = cal_alpha(min_error)                            #########test\n",
    "\n",
    "# 更新数据集的权重 1、规范化因子 Z_m\n",
    "def _Z(clf_result,labels,alpha,weight):\n",
    "    \n",
    "    _z = np.sum(np.array([weight[i] * np.exp( - alpha * labels[i] * clf_result[i] ) for i in range(len(labels))]))\n",
    "    \n",
    "    return _z\n",
    "\n",
    "# _z = _Z(clf_result,labels,_alpha,w0)                 #########test\n",
    "        \n",
    "    \n",
    "# 更新数据集的权重\n",
    "def update_w(weight,z,alpha,labels,clf_result):\n",
    "    new_w = np.array([(weight[i] / z) * np.exp( - alpha * labels[i] * clf_result[i]) for i in range(len(labels))])\n",
    "    print(new_w)\n",
    "    return new_w\n",
    "\n",
    "# update_w(w0,_z,_alpha,labels,clf_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 2\n",
      "epoch: 0\n",
      "min_err: 4.0 best_v: 0.5249999999999999 clf_result: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "alpha: nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan]\n",
      "epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\86150\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "# 组合\n",
    "def fit(X,y):\n",
    "    # 对每个特征进行基本分类器分器，获得误差最小的维度，以及基本分类器\n",
    "    M,N = X.shape    \n",
    "    print(M,N)\n",
    "    \n",
    "    for epoch in range(_iter_num): \n",
    "        min_err,best_v,clf_result = 100000,None,None\n",
    "        print(\"epoch:\",epoch)\n",
    "        \n",
    "        weight = _w[epoch]\n",
    "        \n",
    "        for i in range(N):            \n",
    "            v,error,clf_result = _G(X[:,i],y,weight)\n",
    "            \n",
    "        if error < min_err:\n",
    "            min_err = error \n",
    "            best_v = v \n",
    "            clf_result = clf_result\n",
    "            \n",
    "        if min_err == 0 :\n",
    "            break\n",
    "         \n",
    "        print(\"min_err:\",min_err,\"best_v:\",best_v,\"clf_result:\",clf_result)\n",
    "        \n",
    "        # 计算alpha \n",
    "        alpha = cal_alpha(min_err)\n",
    "        _alpha.append(alpha)\n",
    "        print(\"alpha:\",alpha)\n",
    "        \n",
    "        # 更新分类器\n",
    "        clf_sets.append(clf_result)\n",
    "        \n",
    "        # 计算Z \n",
    "        z = _Z(clf_result,y,alpha,weight)\n",
    "        \n",
    "        # 更新下一个权重\n",
    "        next_w = update_w(weight, z, alpha, y, clf_result)\n",
    "        _w.append(next_w)\n",
    "        \n",
    "\n",
    "fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7141428428542851"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _w = []                 # 存储每一轮学得中，学得到的弱分类器的权重\n",
    "# _alpha = []              # 弱分类器G(x)的系数\n",
    "#clf_sets = []             # 弱分类器集合\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8673005276940532"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5*np.log((1-min_error)/min_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**备注1：** 定义分类器 $\\left\\{\\begin{align}x>v \\Rightarrow +1 \\\\x\\leq v \\Rightarrow -1\\end{align}\\right.$ \n",
    "\n",
    "**备注2：** 弱分类器的误差  <font size=3> $e_m = \\sum_{i=1}^N P(G_m(x_i)\\neq y_i) = \\sum_{i=1}^N w_{mi}I(G_m(x_i)\\neq y_i)$ </font> ， 即被分错的样本的权重之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.3, 5.3, 5.4, 5.8, 5.2, 5.4, 5.7, 5.1, 5.9, 6.1, 5.7, 4.8, 4.8,\n",
       "       4.5, 5.4, 4.4, 5.8, 6.8, 5.6, 4.8, 5.1, 5. , 5.5, 6. , 4.6, 5.6,\n",
       "       6.1, 4.8, 6.9, 5. , 4.7, 6.5, 6.1, 5. , 5. , 5.1, 5.5, 6.1, 4.8,\n",
       "       7. , 4.6, 6.3, 4.9, 5.7, 5.7, 6.2, 5.1, 4.7, 5.1, 5.7, 5.4, 4.4,\n",
       "       4.9, 5.6, 5.8, 5.5, 6.7, 4.4, 5.5, 4.9, 4.6, 6. , 6. , 5.1, 5. ,\n",
       "       5.1, 6.7, 5.6, 5. , 6.3, 5. , 5. , 5.4, 6.6, 5.2, 6.4, 5.5, 5.1,\n",
       "       6.2, 4.9])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max_feature - min_feature + learning_rate)/learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集 $T = \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)\\}$ ，其中 $x_i \\in \\mathcal{X}\\subseteq R^n, y_i\\in\\mathcal{Y} = \\{-1,+1\\}$ ; 弱学习算法。\n",
    "\n",
    "输出：最终分类器 $G(x)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " （3） 构建基本分类器的线性组合\n",
    "#### $$f(x) = \\sum_{m=1}^M\\alpha_mG_m(x)$$\n",
    "得到最终分类器\n",
    "#### $$G(x) = sign(f(x)) = sign\\Big(\\sum_{m=1}^M\\alpha_mG_m(x)\\Big)$$\n",
    "<font><i><b> 线性分组合$f(x)$ 实现$M$ 个基本分类器的加权表决。系数 $\\alpha_m$ 表示了基本分类器 $g_m(x)$ 的重要性。这里，所有$\\alpha_m$ 之后并不为1。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一个特点。</b></i></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
